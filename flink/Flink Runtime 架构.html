<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<link rel="stylesheet" type="text/css" href="css/style.css" />
		<title>Flink Runtime 架构</title>
	</head>
<body>
<h1>Flink Runtime 架构</h1>

<p>针对不同的执行环境，Flink 提供了一套统一的分布式作业执行引擎，也就是Flink Runtime 层，主要架构如下图所示：</p>

<figure><img src="/Users/xerxes/note/flink/src/Flink%E9%9B%86%E7%BE%A4%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84.png"/></figure>

<p>Flink Runtime 层采用了标准的master-slave 架构，master 负责管理整个集群中的资源和作业；slave 也就是TaskManager，负责提供具体的资源并执行作业。</p>

<p>其中master 部分又包含三个组件：</p>

<ul>
	<li><strong>Dispatcher</strong>：负责接收用户提供的作业，并且负责为这个新提交的作业拉起一个新的JobManager 组件；</li>
	<li><strong>ResourceManager</strong>：负责资源的管理，整个Flink 集群只有一个ResourceManager；</li>
	<li><strong>JobManager</strong>：负责管理作业的执行，一个Flink 集群中可能有多个job 同时执行，每个job 都有自己的JobManager 组件</li>
</ul>

<hr />

<h2>运行模式</h2>

<h3>Per-Job 模式</h3>

<p>Flink 支持两种不同的模式，即Session 模式和Per-Job 模式。</p>

<p>Per-Job 模式下整个Flink 集群只执行一个job，每个job 独享Dispatcher 和ResourceManager。</p>

<p>AppMaster 和TaskExecutor 都是按需申请的。</p>

<p>Per-Job 模式更适合运行执行时间较长的job。</p>

<h3>Session 模式</h3>

<p>Session 模式下，Flink 会预先启动AppMaster 以及一组TaskExecutor，然后在整个集群的生命周期中执行多个job。</p>

<p>Session 模式更适合规模小，执行时间段的job。</p>

<hr />

<h2>作业执行步骤</h2>

<ol>
	<li>当用户提交job 时，提交脚本会首先启动一个Clinet 进程负责job 的编译与提交，它首先将用户编写的代码编译为一个JobGraph，还会进行一些检查和优化工作（如判断哪些operator 可以chain 到同一个task 中）；</li>
	<li>然后Client 将产生的JobGraph 提交到集群中执行，此时有两种情况：

		<ul>
			<li>Session 模式：AM 会预先启动，此时Client 直接与Dispatcher 建立连接并提交作业；</li>
			<li>Per-Job 模式：AM 不会预先启动，Client 首先向资源管理系统（如Yarn）申请资源来启动AM，然后再向AM 中的Dispatcher 提交作业；</li>
		</ul></li>
	<li>当作业到Dispatcher 后，Dispatcher 首先启动一个JobManager，然后JobManager 向ResourceManager 申请资源来启动job 中具体的任务，此时有两种情况：

		<ul>
			<li>Session 模式：TaskExecutor 已经启动，此时ResourceManager 中已经记录了TaskExecutor 注册的资源，可以直接选取空闲资源进行分配；</li>
			<li>Per-Job 模式：TaskExecutor 未启动，ResourceManager 需要向外部资源管理系统申请资源来启动TaskExecutor，然后等待TaskExecutor 注册相应资源后再继续选取空闲资源进程分配；</li>
		</ul></li>
	<li>ResourceManager 选取到空闲的slot 之后，就会通知相应的TaskManager 将该slot 分配给JobManager；</li>
	<li>TaskExecutor 进行相应的记录后，向JobManager 进行注册；</li>
	<li>JobManager 收到TaskExecutor 注册的slot 后，就可以实际提交task 了</li>
</ol>

<hr />

<h2>集群角色</h2>

<h3>Tasks and Operator Chains</h3>

<p><strong>对于分布式执行，Flink 会将operator subtasks 连到一个tasks 中，每一个task 由一个线程执行</strong>。</p>

<p>将operators 连到一个task 对于优化非常有用：</p>

<ul>
	<li>减少线程切换和缓存的成本；</li>
	<li>减少延迟的同时增大吞吐</li>
</ul>

<figure><img src="/Users/xerxes/note/flink/src/sample%5C%20dataflow.jpg"/></figure>

<p>如上图是一个dataflow 的例子，该dataflow 有5个subtasks，因此由5个并行线程执行。</p>

<h3>Job Managers，Task Managers，Clients</h3>

<figure><img src="/Users/xerxes/note/flink/src/Flink%E6%9E%B6%E6%9E%84.jpg"/></figure>

<p>Client 提交任务给JobManager，JobManager 再调度任务到各个TaskManager 去执行，然后TaskManager 将心跳和统计信息汇报给JobManager。</p>

<p>TaskManager 之间以流的形式进行数据传输。上述三者都是独立的JVM 进程。</p>

<h4>JobManager（master）</h4>

<p><strong>JobManagers 负责协调分布式执行，调度tasks，coordinate checkpoints，协调故障恢复等</strong>。</p>

<p>集群至少会有一个JobManager。高可用集群会有多个JobManagers，其中一个是leader，其他是standby。</p>

<h4>TaskManager（workers）</h4>

<p><strong>TaskManager 在启动的时候设置好slot，每个slot 启动一个线程，用来执行一个task</strong>。</p>

<p>TaskManager 从JobManager 处接收需要部署的task，部署启动后，与自己的上游建立Netty 连接，接收数据并处理。</p>

<p>集群至少会有一个TaskManager。</p>

<h4>Clients</h4>

<p>Clients 不属于runtime 和程序执行的一部分，Clients 负责准备并发送一个JobGraph 到JobManager。</p>

<p>发送后，client 可以断开连接，或者保持连接来接收程序的报告。</p>

<h3>Dispatcher</h3>

<p>Dispatcher 负责从Client 端接收job 提交请求，并代表Client 在集群管理器上启动作业。</p>

<p>引入Dispatcher 的原因：</p>

<ul>
	<li>一些集群管理器需要一个中心化的job 生成和监控实例；</li>
	<li>能够实现Standalone 模式下JobManager 的角色，等待job 提交</li>
</ul>

<h3>Task Slots and Resources</h3>

<p>每一个TaskManager 都是一个JVM 进程，可能在独立的线程内执行一个或多个subtasks。</p>

<p>为了控制一个worker 接收多少tasks，一个worker 至少会有一个task slot。</p>

<p><strong>每一个task slot 代表一个固定的TaskManager 资源的子集</strong>。</p>

<p>通过调整task slots 的数量，可以定义subtasks 如何与其他subtasks 隔开。每一个TaskManager 有一个slot 意味着每一个task group 运行在一个独立的JVM；有多个slots 意味着多个subtasks 分享相同的JVM，相同的JVM 中的tasks 分享TCP 连接和心跳信息，可能也会分享数据集和数据结构，从而减少每个task 的开销。</p>

<figure><img src="/Users/xerxes/note/flink/src/task%5C%20slot.png"/></figure>

<p><strong>Flink 允许同一个job 的不同tasks 的subtasks 分享同一个slots，一个slot 可以持有一个完整的pipeline</strong>。</p>

<p>这有两个好处：</p>

<ul>
	<li>一个Flink 集群只需要设置和job 的最高并行度一样的task slots，不需要计算程序包含多少个tasks；</li>
	<li>可以更好的利用资源：如果没有slot 共享，那么非密集型操作source/flatmap 就会占用同密集型操作keyAggregation/sink 一样多资源；如果slot 共享，能充分利用slot 资源，同时保证每个TaskManager 能平均分配到比较重的subtasks</li>
</ul>

<figure><img src="/Users/xerxes/note/flink/src/slot%E5%85%B1%E4%BA%AB%E7%A4%BA%E4%BE%8B.jpg"/></figure>

<hr />

<h2>资源管理与作业调度</h2>

<p>作业调度可以看作是对资源和任务进行匹配的过程。在Flink 中，资源是通过slot 来表示的，每个slot 可以用来执行不同的task。任务也就是job 中的task，包含了待执行的用户逻辑。调度的主要目的就是为了给task 找到匹配的slot。</p>

<h3>资源管理</h3>

<figure><img src="/Users/xerxes/note/flink/src/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%8A%9F%E8%83%BD%E6%A8%A1%E5%9D%97%E4%BA%A4%E4%BA%92%E5%85%B3%E7%B3%BB.png"/></figure>

<p>如上图所示，在ResourceManager 中，有一个子组件叫做<strong>SlotManager，它维护了当前集群中所有TaskExecutor 上的slot 的信息与状态</strong>。</p>

<p>当JobManager 来为特定task 申请资源时，根据当前是Per-Job 还是Session 模式，ResourceManager 可能去申请资源来启动新的TaskExecutor。</p>

<p>当新的TaskExecutor 启动之后，它会通过服务发现找到当前活跃的ResourceManager 进行注册，注册信息中包含该TaskExecutor 所有的slot 信息。</p>

<p>ResourceManager 收到注册信息后，其中的SlotManager 就会记录下相应的slot 信息。当JobManager 来为某个task 申请资源时，SlotManager 就会从当前空闲的slot 中按一定规则选择一个空闲的slot 进行分配。</p>

<p>分配完成后，ResourceManager 会首先向TaskManager 发送RPC 要求将选定的slot 分配给特定的JobManager。TaskManager 如果还没有执行过该JobManager 的task 的话，它需要首先向相应的JobManager 建立连接，然后发送提供slot 的RPC 请求。</p>

<p>在JobManager 中，所有task 的请求后缓存到SlotPool 中。当有slot 被提供后，SlotPool 会从缓存的请求中选择相应的请求并结束相应的请求过程。</p>

<p>当task 结束后，无论是正常结束还是异常结束，都会通知JobManager 相应的结束状态，然后在TaskManager 端将slot 标记为已占用但未执行任务的状态。</p>

<p>JobManager 会首先将相应的slot 缓存到SlotPool 中，不会立即释放。这种方式避免了如果将slot 直接还给ResourceManager，在任务异常结束之后需要重新启动，需要重新申请slot 的问题。通过延时释放，Failover 的task 可以尽快调度回原来的TaskManager，从而加快Failover 的速度。</p>

<p>当SlotPool 中缓存的slot 超过指定的时间仍未使用，SlotPool 就会发起释放该slot 的过程。与申请 Slot 的过程对应，SlotPool 会首先通知 TaskManager 来释放该 Slot，然后 TaskExecutor 通知 ResourceManager 该 Slot 已经被释放，从而最终完成释放的逻辑。</p>

<h3>心跳</h3>

<p>除了正常的通信逻辑外，在ResourceManager 和TaskExecutor 之间还存在定时的心跳消息来同步slot 的状态。当组件之间长时间未收到对方的心跳时，就会认为对应的组件已经失效，并进入到Failover 的流程。</p>

<h3>调度顺序</h3>

<p>Flink 提供两种基本的调度逻辑：</p>

<ul>
	<li>Eager：在作业启动时申请资源将所有的task 调度起来</li>
	<li>Lazy From Source：从Source 开始，按拓扑顺序进行调度。先调度没有上游任务的Source 任务，执行完成时将输出数据缓存到内存或者写入磁盘，然后调度后续任务</li>
</ul>

<h3>Flink on Yarn</h3>

<ul>
	<li>Client 提交job 到RM；</li>
	<li>RM 分配第一个container 去运行AM，由AM 负责资源的监督和管理；</li>
	<li>将JobManager 启动在同一个container 里面，做任务调度和分配（Yarn AM 和Flink JobManager 在同一个container 中，这样AM 可以知道Flink JobManager 的地址，从而AM 可以申请container 去启动Flink TaskManager）；</li>
	<li>待Flink 成功运行在Yarn 集群上，Flink Yarn Client 就可以提交Flink Job 到Flink JobManager，并进行后续的映射、调度和计算处理</li>
</ul>

<h4>Flink on Yarn 的缺陷</h4>

<ul>
	<li>资源分配是静态的：一个job 需要在启动时获取所需的资源并且在它的生命周期里一直持有这些资源，这导致了job 不能随负载变化而动态调整；</li>
	<li>所有的container 大小固定，无法根据job 的需求调整container 的结构；</li>
	<li>与容器管理基础设施的交互较笨拙，需要两个步骤启动Flink job：1. 启动Flink 守护进程；2. 提交job；</li>
	<li>job 管理页面会在job 完成后消失不可访问</li>
</ul>

<h4>资源调度模型重构下的Flink on Yarn</h4>

<h5>没有Dispatcher</h5>

<figure><img src="/Users/xerxes/note/flink/src/%E6%B2%A1%E6%9C%89dispatcher.png"/></figure>

<p>Client 提交JobGraph 以及依赖jar 包到YarnResourceManager，Yarn RM 分配第一个container 启动AM，AM 中启动Flink ResourceManager 和JobManager，JobManager 根据JobGraph 生成的ExecutionGraph 以及物理执行计划向Flink ResourceManager 申请slot，Flink ResourceManager 管理这些slot 和请求，如果没有slot 可用就向YarnResourceManager 申请container，container 启动后会注册到Flink ResourceManager，最后JobManager 会将subTask deploy 到对应的container 的slot 中去。</p>

<h5>有Dispatcher</h5>

<figure><img src="/Users/xerxes/note/flink/src/%E6%9C%89dispatcher.png"/></figure>

<p>比没有Dispatcher 增加一个过程，就是Client 会直接通过HTTP Server 的方式，用Dispatcher 将这个任务提交到Yarn ResourceManager。</p>

<hr />

<h2>错误恢复</h2>

<h3>task 执行错误</h3>

<h4>Restart-all</h4>

<p>直接重启所有的task。</p>

<p>对于Flink 的流处理任务，Flink 提供了Checkpoint 机制，因此当任务重启后可以直接从上次的Checkpoint开始继续执行。因此这种方式更适合于流作业。</p>

<h4>Restart-individual</h4>

<p>适用于task 之间没有数据传输的情况。这种情况下，可以直接重启出错的任务。</p>

<h4>Region-Based Failover</h4>

<p>由于 Flink 的批作业没有 Checkpoint 机制，因此对于需要数据传输的作业，直接重启所有 Task 会导致作业从头计算，从而导致一定的性能问题。</p>

<p>为了增强对 Batch 作业，Flink 在1.9中引入了一种新的Region-Based的Failover策略。</p>

<p>在一个 Flink 的 Batch 作业中 Task 之间存在两种数据传输方式：</p>

<ul>
	<li>Pipeline：上下游 Task 之间直接通过网络传输数据，因此需要上下游同时运行；</li>
	<li>Blocking：上游的 Task 会首先将数据进行缓存，因此上下游的 Task 可以单独执行</li>
</ul>

<p>基于这两种类型的传输，Flink 将 ExecutionGraph 中使用 Pipeline 方式传输数据的 Task 的子图叫做 Region，从而将整个 ExecutionGraph 划分为多个子图。可以看出，Region 内的 Task 必须同时重启，而不同 Region 的 Task 由于在 Region 边界存在 Blocking 的边，因此，可以单独重启下游 Region 中的 Task。</p>

<figure><img src="/Users/xerxes/note/flink/src/Region-based%5C%20%E9%94%99%E8%AF%AF%E6%81%A2%E5%A4%8D%E7%AD%96%E7%95%A5%E7%A4%BA%E4%BE%8B%E4%B8%80.png"/></figure>

<p>另一方面，如图如果错误是由于读取上游结果出现问题，如网络连接中断、缓存上游输出数据的 TaskExecutor 异常退出等，那么还需要重启上游 Region 来重新产生相应的数据。在这种情况下，如果上游 Region 输出的数据分发方式不是确定性的（如 KeyBy、Broadcast 是确定性的分发方式，而 Rebalance、Random 则不是，因为每次执行会产生不同的分发结果），为保证结果正确性，还需要同时重启上游 Region 所有的下游 Region。</p>

<figure><img src="/Users/xerxes/note/flink/src/Region-based%5C%20%E9%94%99%E8%AF%AF%E6%81%A2%E5%A4%8D%E7%AD%96%E7%95%A5%E7%A4%BA%E4%BE%8B%E4%BA%8C.png"/></figure>

<h3>集群的Master 出现错误</h3>

<p>除了 Task 本身执行的异常外，另一类异常是 Flink 集群的 Master 进行发生异常。</p>

<p>目前 Flink 支持启动多个 Master 作为备份，这些 Master 可以通过 ZK 来进行选主，从而保证某一时刻只有一个 Master 在运行。</p>

<p>当前活路的 Master 发生异常时,某个备份的 Master 可以接管协调的工作。为了保证 Master 可以准确维护作业的状态，Flink 目前采用了一种最简单的实现方式，即直接重启整个作业。</p>

</body>
</html>

