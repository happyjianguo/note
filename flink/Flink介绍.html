<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<link rel="stylesheet" type="text/css" href="css/style.css" />
		<title>Flink介绍</title>
	</head>
<body>
<h1>Flink介绍</h1>

<blockquote>
<p>Flink是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态或无状态的计算，能够部署在各种集群环境，对各种规模大小的数据进行快速计算。</p>
</blockquote>

<hr />

<h2>Flink 相关概念</h2>

<h3>Streams</h3>

<figure><img src="/Users/xerxes/note/flink/src/streams.jpg"/></figure>

<p>流，分为有限数据流和无限数据流。</p>

<ul>
	<li>bounded stream是限定大小的有始有终的数据集合；</li>
	<li>unbounded stream是有始无终的数据流。</li>
</ul>

<p>二者的区别在于无限数据流的数据会随时间的推演而持续增加，计算持续进行且不存在结束的状态；相对的有限数据流数据大小固定，计算最终会完成并处于结束的状态。</p>

<h3>State</h3>

<figure><img src="/Users/xerxes/note/flink/src/state.jpg"/></figure>

<p>状态是计算过程中的数据信息，在容错恢复和CheckPoint中有重要的作用，流计算在本质上是Incremental Processing，因此需要不断查询保持状态。</p>

<p>另外，为了确保Exactly-once语义，需要数据能够写入到状态中；而持久化存储，能够保证在整个分布式系统运行失败或者挂掉的情况下做到Exactly-once，这是状态的另一个价值。</p>

<h3>Time</h3>

<figure><img src="/Users/xerxes/note/flink/src/time.jpg"/></figure>

<p>Flink 的无限数据流是一个持续的过程，时间是我们判断业务状态是否滞后，数据处理是否及时的重要依据。Flink 中的时间有三种：</p>

<ul>
	<li>Event Time：事件发生的时间，是一个业务时间；</li>
	<li>Ingestion Time：时间进入到Flink系统的时间；</li>
	<li>Processing Time：每个算子处理数据的时间</li>
</ul>

<h3>API</h3>

<figure><img src="/Users/xerxes/note/flink/src/API.jpg"/></figure>

<p>API通常分为三层，由上而下分别为：</p>

<ul>
	<li>SQL/Table API</li>
	<li>DataStream API</li>
	<li>ProcessFunction</li>
</ul>

<p>API的表达能力及业务抽象能力都非常强大，但也接近SQL层，表达能力会逐步减弱，抽象能力会增强，反之，ProcessFunction层API表达能力强，可以进行多种灵活方便的操作，但抽象能力相对较小。</p>

<hr />

<h2>Flink 架构</h2>

<h3>架构特点</h3>

<p>在架构方面，主要分为四点：</p>

<ul>
	<li>有界和无界数据流：Flink具备统一的框架处理有界和无界两种数据流的能力；</li>
	<li>部署灵活：Flink底层支持多种资源调度器，包括Yarn、Kubernetes等，并且自身带Standalone的调度器，在部署上也十分灵活；</li>
	<li>极高的可伸缩性：可伸缩性对于分布式系统十分重要，阿里巴巴双十一大屏采用Flink处理海量数据，使用过程中测得Flink峰值可达17亿/秒；</li>
	<li>极致的流式处理性能：Flink将状态语义完全抽象到框架中，支持本地状态读取，避免了大量网络I/O，可以极大提升状态存取的性能</li>
</ul>

<h3>架构原理</h3>

<figure><img src="/Users/xerxes/note/flink/src/Flink%E6%9E%B6%E6%9E%84.jpg"/></figure>

<p>Flink 集群启动后，首先会启动一个<strong>JobManager</strong>，和一个或多个<strong>TaskManager</strong>。</p>

<p>Client 提交任务给JobManager，JobManager 再调度任务到各个TaskManager 去执行，然后TaskManager 将心跳和统计信息汇报给JobManager。</p>

<p>TaskManager 之间以流的形式进行数据传输。上述三者都是独立的JVM 进程。</p>

<ul>
	<li><code>Client</code>：提交job 的客户端，提交job 后，可以结束进程，也可以不结束并等待结果返回；</li>
	<li><code>JobManager</code>：主要负责调度job 并协调task 做checkpoint。从<code>Client</code>处接收到job 和jar 包等资源后，会生成优化后的执行计划，并以task 为单元调度到各个<code>TaskManager</code>去执行；</li>
	<li><code>TaskManager</code>：在启动的时候设置好Slot，每个Slot 启动一个线程，用来执行一个task。从<code>JobManager</code>处接收需要部署的task，部署启动后，与自己的上游建立Netty 连接，接收数据并处理。</li>
</ul>

<h4>Task Slot</h4>

<p><strong>Task slot 代表了TaskManager 的一个固定大小的资源子集</strong>。例如一个拥有三个slot 的TaskManager，会将其管理的内存平均分成三份分给slot。</p>

<p>通过调整task slot 的数量，用户可以定义task 之间是如何相互隔离的。每个TaskManager 有一个slot，也就意味着每个task 运行在独立的JVM 中；每个TaskManager 有多个slot 的话，也就意味着多个task 运行在同一个JVM 中，而同一个JVM 进程中的task可以共享TCP 连接（基于多路复用）和心跳信息，可以减少数据的网络传输，也能共享一些数据结构，一定程度上减少每个task 的消耗。</p>

<p><strong>每一个TaskManager 会拥有一个或多个task slot，每个slot 都能跑由多个连续task 组成的一个pipeline</strong>，比如MapFunction 的第n 哥并行实例和ReduceFunction 的第n 个并行实例可以组成一个pipeline。</p>

<h5>SlotSharingGroup和 CoLocationGroup</h5>

<p><strong>默认情况下，Flink 允许subtasks 共享slot，条件是它们都来自同一个job 的不同task</strong>。结果可能一个slot 持有该job 的整个pipeline。</p>

<p>允许slot 共享有两个好处：</p>

<ul>
	<li>FLink 集群所需的task slot 数与job 中最高的并行度一致，也就是说我们不需要再去计算一个程序总共会起多少个task；</li>
	<li>更容易获得更充分的资源利用。如果没有slot 共享，那么非密集型操作source/flatmap 就会占用同密集型操作keyAggregation/sink 一样多资源；如果slot 共享，将基线的两个并行度增加到六个，能充分利用slot 资源，同时保证每个TaskManager 能平均分配到重的subtasks。</li>
</ul>

<figure><img src="/Users/xerxes/note/flink/src/slot%E5%85%B1%E4%BA%AB%E7%A4%BA%E4%BE%8B.jpg"/></figure>

<p>如图表示并行度为6，并开启slot 共享得到的slot 分布图。</p>

<hr />

<h2>程序拓扑</h2>

<p>Flink 中的执行图可以分成四层：StreamGraph、JobGraph、ExecutionGraph、物理执行图。</p>

<h3>StreamGraph</h3>

<figure><img src="/Users/xerxes/note/flink/src/StreamGraph.jpg"/></figure>

<p>根据用户通过Stream API 编写的代码生成的最初的图，用来表示程序的拓扑结构；</p>

<ul>
	<li>StreamNode：用来代表operator，并具有所有相关的属性，如并发度、入边和出边等；</li>
	<li>StreamEdge：表示连接两个StreamNode 的边；</li>
</ul>

<h3>JobGraph</h3>

<figure><img src="/Users/xerxes/note/flink/src/JobGraph.jpg"/></figure>

<p><code>StreamGraph</code>经过优化后生成<code>JobGraph</code>，提交给<code>JobManager</code>；</p>

<ul>
	<li>JobVertex：经过优化后符合条件的多个StreamNode 可能会chain 在一起生成一个JobVertex，即一个JobVertex 包含一个或多个operator，JobVertex 的输入是JobEdge，输出是IntermediateDataSet；</li>
	<li>JobEdge：代表了JobGraph 中的一条数据传输通道，source 是IntermediateDataSet，target 是JobVertex。即数据通过JobEdge由IntermediateDataSet传递给目标JobVertex；</li>
	<li>IntermediateDataSet：表示JobVertex 的输出，即经过operator 处理产生的数据集。producer 是JobVertex，consumer 是JobEdge；</li>
</ul>

<h3>ExecutionGraph</h3>

<figure><img src="/Users/xerxes/note/flink/src/ExecutionGraph.jpg"/></figure>

<p><code>JobManager</code>根据<code>JobGraph</code>生成<code>ExecutionGraph</code>，它是<code>JobGraph</code>的并行化版本，是调度层最核心的数据结构；</p>

<ul>
	<li>ExecutionJobVertex：和JobGraph中的JobVertex一一对应。每一个ExecutionJobVertex都有和并发度一样多的 ExecutionVertex；</li>
	<li>ExecutionVertex：表示ExecutionJobVertex的其中一个并发子任务，输入是ExecutionEdge，输出是IntermediateResultPartition；</li>
	<li>IntermediateResult：和JobGraph中的IntermediateDataSet一一对应。一个IntermediateResult包含多个IntermediateResultPartition，其个数等于该operator的并发度；</li>
	<li>IntermediateResultPartition：表示ExecutionVertex的一个输出分区，producer是ExecutionVertex，consumer是若干个ExecutionEdge；</li>
	<li>ExecutionEdge：表示ExecutionVertex的输入，source是IntermediateResultPartition，target是ExecutionVertex。source和target都只能是一个；</li>
	<li>Execution：是执行一个 ExecutionVertex 的一次尝试。当发生故障或者数据需要重算的情况下 ExecutionVertex 可能会有多个 ExecutionAttemptID。一个 Execution 通过 ExecutionAttemptID 来唯一标识。JM和TM之间关于 task 的部署和 task status 的更新都是通过 ExecutionAttemptID 来确定消息接受者；</li>
</ul>

<h3>物理执行图</h3>

<figure><img src="/Users/xerxes/note/flink/src/%E7%89%A9%E7%90%86%E6%89%A7%E8%A1%8C%E5%9B%BE.jpg"/></figure>

<p><code>JobManager</code>根据<code>ExecutionGraph</code>对job 进行调度后，在各个<code>TaskManager</code>上部署task 后形成的图，并不是一个具体的数据结构；</p>

<ul>
	<li>task：Execution被调度后在分配的 TaskManager 中启动对应的 Task。Task 包裹了具有用户执行逻辑的 operator；</li>
	<li>ResultPartition：代表由一个Task的生成的数据，和ExecutionGraph中的IntermediateResultPartition一一对应；</li>
	<li>ResultSubpartition：是ResultPartition的一个子分区。每个ResultPartition包含多个ResultSubpartition，其数目要由下游消费 Task 数和 DistributionPattern 来决定；</li>
	<li>InputGate：代表Task的输入封装，和JobGraph中JobEdge一一对应。每个InputGate消费了一个或多个的ResultPartition；</li>
	<li>InputChannel：每个InputGate会包含一个以上的InputChannel，和ExecutionGraph中的ExecutionEdge一一对应，也和ResultSubpartition一对一地相连，即一个InputChannel接收一个ResultSubpartition的输出</li>
</ul>

<hr />

<h2>task&amp;operator chain</h2>

<p><strong>为了高效地分布式执行，Flink 会尽可能将operator 子任务chain 在一起形成task，每个task 在一个线程中执行</strong>。</p>

<p>将operator 链接成task 是非常有效的优化：</p>

<ul>
	<li>减少线程切换；</li>
	<li>减少消息的序列化/反序列化；</li>
	<li>减少数据在缓冲区的交换；</li>
	<li>减少延迟；</li>
	<li>提高吞吐量</li>
</ul>

<h3>operator 链接条件</h3>

<ul>
	<li>上下游并行度一致；</li>
	<li>下游节点的入度为1，也就是下游节点没有来自其他节点的输入；</li>
	<li>上下游节点都在同一个slot group 中；</li>
	<li>下游节点的chain 策略为ALWAYS（可以与上下游链接，map、flatmap、filter等默认是ALWAYS）；</li>
	<li>上游节点的chain 策略为ALWAYS/HEAD（只能与下游链接，不能与上游链接，source 默认是HEAD）；</li>
	<li>两个节点间数据分区方式是forward；</li>
	<li>用户未禁止chain</li>
</ul>

<hr />

<h2>Flink Operation</h2>

<p>Flink 关于运维及业务监控的内容：</p>

<ul>
	<li>Flink具备7*24小时高可用的SOA（面向服务架构），原因是在实现上Flink提供了一致性的Checkpoint。Checkpoint是Flink实现容错机制的核心，它周期性的记录计算过程中Operator的状态，并生成快照持久化存储。当Flink作业发生故障崩溃时，可以有选择的从Checkpoint中恢复，保证了计算的一致性；</li>
	<li>Flink本身提供监控、运维等功能或接口，并有内置的WebUI，对运行的作业提供DAG图以及各种Metric等，协助用户管理作业状态</li>
</ul>

<hr />

<h2>Flink 的应用场景</h2>

<h3>Data Pipeline</h3>

<figure><img src="/Users/xerxes/note/flink/src/Data%5C%20Pipeline.jpg"/></figure>

<p>Data Pipeline的核心场景类似于数据搬运并在搬运过程中进行部分数据清洗或者处理。Flink提供了流式ETL或者实时ETL，能够订阅消息队列的消息并进行处理，清洗完成后实时写入到下游的Database或File System中。</p>

<p>有以下几个场景：</p>

<h4>实时数仓</h4>

<p>当下游要构建实时数仓时，上游则需要实时的Stream ETL。这个过程会进行实时清洗或扩展数据，清洗完成后写入到下游的实时数据的整个链路中，可保证数据查询的时效性，形成实时数据的采集、实时数据处理以及下游的实时query。</p>

<h4>搜索引擎推荐</h4>

<p>当卖家上线新商品时，后台会实时产生消息流，该消息流经过Flink系统时会进行数据的处理、扩展。然后将处理及扩展后的数据生成实时索引，写入到搜索引擎中。这样就能在秒级或者分钟级实现搜索引擎的搜索。</p>

<h3>Data Analytics</h3>

<figure><img src="/Users/xerxes/note/flink/src/Data%5C%20Analytics.jpg"/></figure>

<h3>Data Driven</h3>

<figure><img src="/Users/xerxes/note/flink/src/Data%5C%20Driven.jpg"/></figure>

<p>从某种程度上来说，所有的实时的数据处理或者是流式数据处理都属于Data Driven。流计算本质上是Data Driven计算。</p>

<p>应用较多如风控系统需要处理各种各样复杂的规则时，Data Driven就会把处理的规则和逻辑写入到Datastream API或者ProcessFunction API中，然后将逻辑抽象到整个Flink引擎中，当外面的数据流或者是事件进入就会触发相应的规则，这就是Data Driven的原理。</p>

<p>触发规则后，Data Driven会进行处理或者是进行预警，这些预警会发到下游产生业务通知，这是Data Driven的应用场景，Data Driven在应用上更多应用于复杂事件的处理。</p>

</body>
</html>

