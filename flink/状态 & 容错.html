<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<link rel="stylesheet" type="text/css" href="css/style.css" />
		<title>状态 &amp; 容错</title>
	</head>
<body>
<h1>状态 &amp; 容错</h1>

<hr />

<h2>介绍</h2>

<h3>状态</h3>

<p>Stateful functions 和operators 在处理事件时会为状态存储数据，使状态成为任何更加复杂操作的关键building block。如：</p>

<ul>
	<li>状态会存储应用目前为止的事件序列，以便应用寻找事件；</li>
	<li>每分钟/小时/天进行一次聚合时，也就是进行窗口计算时，状态会持有窗口触发计算之前的数据；</li>
	<li>在数据流上训练机器学习模型时，状态持有当前训练的模型以及当前模型的参数；</li>
	<li>需要管理历史数据时，状态允许高效访问过去发生的事件</li>
</ul>

<h3>容错</h3>

<p><strong>Flink 的容错机制的核心部分是制作分布式数据流和操作算子状态的一致性快照</strong>。这些快照充当一致性checkpoint，系统可以在发生故障时回滚。</p>

<p>容错需要做到：</p>

<ul>
	<li>确保状态拥有<strong>exactly-once guarantee</strong>的容错保证；</li>
	<li>在分布式场景下替多个拥有本地状态的算子产生一个<strong>全域一致的快照（global consistent snapshot）</strong>；</li>
	<li>在不中断运算的前提下产生快照</li>
</ul>

<hr />

<h2>State 的类型</h2>

<p>Flink 有两种基础的state：</p>

<ul>
	<li>Keyed State</li>
	<li>Operator State</li>
</ul>

<p>Keyed State 和Operator State 存在两种形式：</p>

<ul>
	<li>Managed State</li>
	<li>Raw State</li>
</ul>

<h3>Operator State</h3>

<p><strong>一个operator state 和一个并发的operator instance 绑定</strong>。即如果operator 的并行度为2，那么其应该有两个对应的operator state。</p>

<p>一个operator 的状态不能被其他operator 所访问到。</p>

<p>当程序的并行数改变时，Operator State 接口支持在并行算子实例之间重新分发状态。</p>

<h3>Keyed State</h3>

<p><strong>可以把Keyed State 当作根据key 值进行分区的Operator State，每个key 对应一个分区</strong>。</p>

<p>Keyed State 总是和key 相关，并且只能用在KeyedStream 上的functions 和operators。</p>

<p>每一个Keyed-state 逻辑上和一个唯一的&lt;parallel-operator-instance, key&gt;绑定，由于<strong>每一个key 属于一个keyed operator 的一个并行实例</strong>，因此可以把一个Keyed-state 当作&lt;operator, key&gt;。</p>

<p>Keyed State 可以进一步组织成Key Groups。Key Groups 是Flink 重新分发Keyed State 的一个原子单元；Key Group 的数量和最大并行数相同。</p>

<p>在运行时，<strong>每一个keyed operator 的并行实例处理一个或多个Key Groups</strong>。</p>

<h3>Managed State &amp; Raw State</h3>

<p>Manager State 由Flink 运行时控制的数据结构（如内部哈希表、RocksDB）表示，如ValueState、ListState。Flink 运行时将状态编码，写入checkpoints。</p>

<p>Raw State 是operators 在它们自己的数据结构中维护的状态。当发生checkpoint 是，它们只是写一个字节序列在checkpoint 中，Flink 并不知道状态的数据结构，只能看到字节码。</p>

<p>所有数据流函数都可以使用Managed State，但是Raw State 只有在实现了operators 时才可以使用。</p>

<p>官方更推荐使用Managed State，因为当并行度改变时，Flink 可以自动重新分发状态。而且对内存的管理也更好。</p>

<hr />

<h2>State 的使用</h2>

<h3>Using Managed Keyed State</h3>

<p>Managed keyed state 接口提供了对不同state 类型的访问，这些state 的作用域为当前输入元素的key。也就是说Keyed State 只能用在KeyedStream 上。</p>

<h4>Keyed Stream 的类型</h4>

<ul>
	<li>ValueState&lt;T&gt;：存储单值类型的state。可以使用<code>update(T)</code>进行更新，<code>T value()</code>进行检索；</li>
	<li>ListState&lt;T&gt;：存储一个list。可以使用<code>add(T)</code>或<code>addAll(list&lt;T&gt;)</code>添加元素，使用<code>Iterable&lt;T&gt; get()</code>检索一个Iterable，也可以使用<code>update(List&lt;T&gt;)</code>重写一个list；</li>
	<li>ReducingState&lt;T&gt;：存储一个单值，该值代表所有添加到state 中的值的聚合。接口类似于ListState&lt;T&gt;，但是使用<code>add(T)</code>添加的元素会通过指定的<code>ReduceFunction</code>来reduce 为一个聚合；</li>
	<li>AggregatingState&lt;IN, OUT&gt;：存储一个单值，该值代表所有添加到state 中的值的聚合。与<code>ReducingState</code>相反，聚合类型可能与state 中的元素类型不同；</li>
	<li>FoldingState&lt;T, ACC&gt;：deprecated in Flink 1.4；</li>
	<li>MapState&lt;UK, UV&gt;：存储一个mappings 的列表。可以将key-value 键值对put 进去，并在所有当前存储的mappings 上检索一个iterable</li>
</ul>

<p>所有类型的state 都有一个<code>clear()</code>方法，用于清除当前活动的key 的state，即输入元素的key。</p>

<p>这些state 对象只能用于与state 交互。State 不需要一定要存储在内部，也可能存储在磁盘或其他地方。</p>

<p>从state 获取值依赖于输入元素的key，因此如果涉及的key 不同，那么在一次用户函数中调用得到的值可能与另一次调用得到的值不相同。</p>

<h4>使用</h4>

<p><strong>要得到一个state 句柄，首先要创建一个<code>StateDescriptor</code></strong>。它持有一个state 的名字，和state type，可能还持有一个user-specified function 如<code>ReduceFunction</code>。</p>

<p>取决于你想要检索什么类型的state，可以创建不同的<code>StateDescriptor</code>：<code>ValueStateDescriptor</code>,<code>ListStateDescriptor</code>,<code>ReducingStateDescriptor</code>, <code>FoldingStateDescriptor&nbsp;</code>or <code>MapStateDescriptor</code></p>

<p><strong>State 通过<code>RuntimeContext</code>访问，因此只能在rich functions 中使用</strong>。<code>RichFunction</code>中的<code>RuntimeCOntext</code>又如下方法访问state：</p>

<ul>
	<li><code>ValueState&lt;T&gt; getState(ValueStateDescriptor&lt;T&gt;)</code></li>
	<li><code>ReducingState&lt;T&gt; getReducingState(ReducingStateDescriptor&lt;T&gt;)</code></li>
	<li><code>ListState&lt;T&gt; getListState(ListStateDescriptor&lt;T&gt;)</code></li>
	<li><code>AggregatingState&lt;IN, OUT&gt;  getAggregatingState(AggregatingStateDescriptor&lt;IN, ACC, OUT&gt;)</code></li>
	<li><code>FoldingState&lt;T, ACC&gt; getFoldingState(FoldingStateDescriptor&lt;T, ACC&gt;)</code></li>
	<li><code>MapState&lt;UK, UV&gt; getMapState(MapStateDescriptor&lt;UK, UV&gt;)</code></li>
</ul>

<h4>State Time-To-Live (TTL)</h4>

<p>任何类型的keyed state 都可以分配一个TTL。如果配置了TTL 并且state 的值过期了，这个值就会被清除。</p>

<p>所有state 集合类型都支持per-entry 设置TTL，这意味着列表元素和map entry 独立过期。</p>

<p>为了使用state TTL 首先必须build 一个<code>StateTtlConfig</code>。TTL 功能可以在任何state descriptor 中通过传递配置启用：</p>

<pre><code>StateTtlConfig ttlConfig = StateTtlConfig
	.newBuilder(Time.seconds(1))
	.setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)
	.setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)
	.build();
ValueStateDescriptor&lt;String&gt; stateDescriptor = new ValueStateDescriptor&lt;&gt;(&quot;state&quot;, String.class);
stateDescriptor.enableTimeToLive(ttlConfig);
</code></pre>

<p>TTL 配置有如下这些选项需要注意：</p>

<ul>
	<li><code>newBuilder()</code>的第一个参数是强制的，代表time-to-live 的值；</li>
	<li>update type 配置了何时刷新状态：

		<ul>
			<li><code>StateTtlConfig.UpdateType.OnCreateAndWrite</code>：默认选项，只在创建和写访问时；</li>
			<li><code>StateTtlConfig.UpdateType.OnReadAndWrite</code>：也在读访问时</li>
		</ul></li>
	<li>state visibility 配置了在读访问时是否返回过期值（如果该值还没有被清除）：

		<ul>
			<li><code>StateTtlConfig.StateVisibility.NeverReturnExpired</code>：默认选项，永远不返回；</li>
			<li><code>StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp</code>：如果还存在就返回</li>
		</ul></li>
</ul>

<h3>Using Managed Operator State</h3>

<p><strong>Stateful function 可以通过实现更通用的<code>CheckpointedFunction</code>接口或者<code>ListCheckpointed&lt;T extends Serializable&gt;</code>接口来使用managed operator state</strong>。</p>

<h4>CheckpointedFunction</h4>

<p>CheckpointedFunction 接口通过不同redistribution schemes 提供了对non-keyed state 的访问，它需要实现两个方法：</p>

<pre><code>void snapshotState(FunctionSnapshotContext context) throws Exception;
void initializeState(FunctionInitializationContext context) throws Exception;
</code></pre>

<p>不管何时要做一次checkpoint 的时候都会调用<code>snapshotState()</code>。</p>

<p>每次初始化user-defined function 时都会调用<code>initializeState()</code>，无论是在函数第一次出实话时还是在函数从之前的checkpoint 恢复时。因此，<code>initializeState()</code>不只是不同类型state 初始化的地方，还是包含state 恢复逻辑的地方。</p>

<h4>ListCheckpointed</h4>

<p>ListCheckpointed 接口是CheckpointedFunction 的一个更有限的变体，它只支持带有偶数分割redistribution 方案的list-style state。它也需要实现两个方法：</p>

<pre><code>List&lt;T&gt; snapshotState(long checkpointId, long timestamp) throws Exception;
void restoreState(List&lt;T&gt; state) throws Exception;
</code></pre>

<p>Operator 要在<code>snaoshotState()</code>方法中向checkpoint 返回一个对象列表，<code>restoreState()</code>必须在恢复时处理这个列表。如果state 不可重分区，则可以始终返回一个<code>Collections.singletonList(MY_STATE)</code>。</p>

<h4>Stateful Source Functions</h4>

<p>与其他operator 相比，Stateful Source 需要多加注意。为了使state 和输出集合的更新是原子的（失败/恢复时需要exactly-once 语义），用户需要从source 的context 中得到一个锁。</p>

<hr />

<h2>容错机制</h2>

<p>Flink 提供了容错机制来一致地恢复流处理程序的状态。该机制保证即使出现错误，程序的状态最终也会以exactly once 的语义反映数据流中的每一条记录。</p>

<h3>Checkpointing</h3>

<p>Flink 为了做到状态容错，需要对状态做checkpoint。Checkpoints 允许Flink 恢复状态和stream 的位置来给应用与无故障相同的语义。</p>

<p>Flink 容错机制的核心是绘制分布式数据流和operator state的一致性snapshot。这些snapshots 作为一致性checkpoint，当程序发生故障时可以退回到这些checkpoint。</p>

<h4>前提</h4>

<p>Flink 的checkpoint 机制会和streams 和state 的持久化存储交互。</p>

<p>一般来说它需要：</p>

<ul>
	<li>能在一定时间内重放记录的数据源，如持久化消息队列Kafka，或者文件系统HDFS；</li>
	<li>持久化的状态存储，如分布式文件系统HDFS</li>
</ul>

<h4>启用和配置Checkpointing</h4>

<p>Checkpoint 默认情况下是关闭的，需要在<code>StreamExecutionEnvironment</code>上调用<code>enableCheckpointing(n)</code>来开启，n 代表checkpoint 的间隔。</p>

<p>checkpointing 的其他参数包括：</p>

<ul>
	<li>exactly-once vs. at-least-once

		<p>可以在调用<code>enableCheckpointing(n)</code>方法时选择语义。大多数应用最好使用exactly-once；at-least-once 更多用在低延迟应用上；</p></li>
	<li>checkpoint timeout</li>
	<li>checkpoints 之间最小间隔时间</li>
	<li>checkpoint 的并发数</li>
	<li>externalized checkpoints</li>
	<li>checkpoint error 上的失败/继续任务</li>
	<li>优先选择checkpoint 进行恢复

		<p>这取决于一个job 是否回退到最近的checkpoint，即使有更近的savepoint 可能缩短恢复时间</p></li>
</ul>

<pre><code>StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// start a checkpoint every 1000 ms
env.enableCheckpointing(1000);

// advanced options:

// set mode to exactly-once (this is the default)
env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);

// make sure 500 ms of progress happen between checkpoints
env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);

// checkpoints have to complete within one minute, or are discarded
env.getCheckpointConfig().setCheckpointTimeout(60000);

// allow only one checkpoint to be in progress at the same time
env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);

// enable externalized checkpoints which are retained after job cancellation
env.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);

// allow job recovery fallback to checkpoint when there is a more recent savepoint
env.getCheckpointConfig().setPreferCheckpointForRecovery(true);
</code></pre>

<h4>barriers</h4>

<p>Flink分布式快照的核心之一是barrier。<strong>这些barrier被注入数据流，并与记录一起作为数据流的一部分向下流动</strong>。barrier永远不会超过记录，严格有序。</p>

<p><strong>Barrier将数据流中的记录分为进入当前快照的记录和进入下一个快照的记录</strong>。每个barrier都带有快照的id，barrier之前的记录都属于该快照。</p>

<p>Barrier不会中断数据流的流动，非常轻量。来自不同快照的多个barrier可以同时出现在数据流中，这意味着可以同时发生多个快照。</p>

<figure><img src="/Users/xerxes/note/flink/src/%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E6%96%B9%E6%B3%95.jpg"/></figure>

<p><strong>Barrier在数据流源处被注入并行数据流中，快照N的barrier被插入的位置是快照N所包含的数据在数据源中最大位置</strong>。例如在Kafka 中，此位置将是分区中最后一条记录的offset，将该位置报告给checkpoint 协调器，也就是Flink的JobManager。</p>

<p><strong>barrier向下游流动，当一个中间operator 从其所有输入流中收到快照N的barrier时，它会为快照N向所有输出流发出一个barrier。一旦sink operator 从其所有输入流接收到barrier N，它就向checkpoint 协调器确认快照N完成。当所有sink确认快照后，意味着快照已完成</strong>。</p>

<p>一旦完成快照N，job 将永远不再向数据源请求快照N之前的记录，因为此时这些记录已经通过整个数据流拓扑，已经被处理结束。</p>

<figure><img src="/Users/xerxes/note/flink/src/%E5%AF%B9%E9%BD%90%E8%BE%93%E5%85%A5%E6%B5%81.jpg"/></figure>

<p><strong>从多个输入流接收数据的operator 需要在snapshot 上对齐streams</strong>，如上图展示了这个过程：</p>

<ol>
	<li>当operator 从一个输入流接收到snapshot barrier N，operator 不会进一步处理这个流中的数据，直到operator 从其他输入流接收到barrier N。另外，operator 会将snapshot N 和snapshot N+1 的数据混合；</li>
	<li>报告barrier N 的数据流会被暂时搁置，从这些数据流接收到的数据不会被处理，而是放在input buffer 中；</li>
	<li>一旦最后一个stream 接收到barrier N，operator 发出所有挂起的数据，然后发出snapshot barrier N；</li>
	<li>之后，operator 会处理input buffer 中的数据，然后继续处理输入流的数据</li>
</ol>

<h4>State</h4>

<p>当operator 包含任何形式的state 时，state 也必须是snapshot 的一部分。</p>

<p>operator state 具有不同的形式：</p>

<ul>
	<li>User-defined state：直接由transformation functions（如map()、filter()）创造并修改的state；</li>
	<li>System state：指的是属于operator 的计算的一部分的data buffer。如window buffers，系统会收集（并聚合）window 中的数据，直到window 被计算和清除</li>
</ul>

<p>在operator 从所有输入流中接收到所有snapshot barriers 之后并在将snapshot barriers 发出到输入流之前，会为它们的state 做一次snapshot。</p>

<p>此时，barriers 之前的数据对state 所做的更新都已完成，而且当barriers 被应用之后，不会再有依赖于数据的更新。state 被存储后，operator 确认checkpoint，然后发出snapshot barrier 到输出流。</p>

<p>此时snapshot 的结果包含：</p>

<ul>
	<li><strong>对于每一个并行的源数据流，当snapshot 启动时在流中的offset/position</strong>；</li>
	<li><strong>对于每一个operator，指向被存储为snapshot 的一部分的状态的指针</strong></li>
</ul>

<figure><img src="/Users/xerxes/note/flink/src/checkpoint%5C%20%E6%B5%81%E7%A8%8B.jpg"/></figure>

<h4>Exactly Once vs. At Least Once</h4>

<p>对齐streams 的步骤可能会增加程序的延迟，对于需要非常低延迟的程序，Flink 可以跳过对齐步骤。当operator 接收到来自每个输入流的checkpoint barrier 时，仍然会绘制snapshot。</p>

<p>当对齐streams 步骤被跳过时，operator 持续处理所有输入，即使checkpoint N 的barrier 到来。这样一来，operator 也可以在获取checkpoint N 的state snapshot 之前处理checkpoint N+1 的数据。在恢复时，这部分数据会重复，因为它们都包含在checkpoint N 的state snapshot 中，会在checkpoint N 之后重放。</p>

<p>对齐streams 只会发生在有多个predecessors 的，或者有多个sender 的operator中。因此，对于只有并行流操作（如map()、flatMap()、filter()）的数据流，即使在At Least Once 模式下，也可以保证Exactly Once。</p>

<h4>异步state snapshot</h4>

<p>operator 在存储state snapshot 到state backend 时会停止处理数据，这个同步存储state snapshot 的过程会在每次捕获snapshot 时发生延迟。</p>

<p>异步存储state snapshot，就可以使operator 在存储state snapshot 时继续处理数据。为此operator 必须能够生产一个state object，state object 应该以一种之后对operator state 修改不会影响这个state object 的方式存储。如RocksDB 使用的copy-on-write 数据结构就可以这样。</p>

<p>接收到checkpoint barrier 之后，operator 开始异步将snapshot 拷贝到state，然后立即将barrier 发给输出流，并且持续处理数据。一旦拷贝完成，就向checkpoint coordinator（JobManager）确认这个checkpoint。checkpoint 现在只能在所有sink 接收到barrier 并且所有stateful operators 确认它们的拷贝完成之后，才会完成。</p>

<h3>分布式快照流程</h3>

<figure><img src="/Users/xerxes/note/flink/src/%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E6%B5%81%E7%A8%8B-1.jpg"/></figure>

<p>假设现在需要产生checkpoint barrier N，Flink的JobManager会触发checkpoint，checkpoint被触发后从数据源产生checkpoint barrier N，checkpoint barrier N 逐步填充左下角表格。</p>

<figure><img src="/Users/xerxes/note/flink/src/%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E6%B5%81%E7%A8%8B-2.jpg"/></figure>

<p>如上图所示红色的事件都由红色的checkpoint barrier N 负责。当数据源收到checkpoint barrier N 之后会先将自己的状态保存（以Kafka为例这个状态就是Kafka分区的位置），之后下游的operator 1会开始运算属于checkpoint barrier N 的数据，当checkpoint barrier N 跟着这些数据流动到operator 1之后，operator 1也将属于checkpoint barrier N 的所有数据都反映在状态中，并写到checkpoint N中。</p>

<figure><img src="/Users/xerxes/note/flink/src/%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E6%B5%81%E7%A8%8B-3.jpg"/></figure>

<p>快照完成后继续往下游走，operator 2也会接收到所有数据，然后搜索checkpoint barrier N 的数据并反映到状态，当状态收到checkpoint barrier N 之后也会写到checkpoint N 中。</p>

<p>以上过程可以看到checkpoint barrier N 已经完成了一个完整的表格，这个表格叫做Distributed Snapshots，即分布式快照。</p>

<h3>恢复</h3>

<p>发生故障后，Flink 选择一个最近的完整的checkpoint。</p>

<p>系统重新部署整个分布式数据流，然后将checkpoint 的snapshot 的一部分state 发送给每一个operator。Source 开始从checkpoint 给的位置读取数据流。</p>

<p>如果state 是被增量快照的，operators 从最近的完整的snapshot</p>

<p> 开始，然后将一系列增量snapshot 应用在state 上。</p>

<hr />

<h2>状态后端</h2>

<p>Flink 的checkpoint 机制存储所有stateful operators 和timers 的一致性snapshot，包括connectors，windows 和任何user-defined state。</p>

<p>Flink 提供不同的存储状态的状态后端。</p>

<p>状态可以位于Java 的堆内和堆外。根据你的状态后端，Flink 也可以为应用管理状态，这意味着Flink 管理内存（如有必要会溢出到磁盘）以允许应用持有大量的状态。</p>

<p>默认情况下state 存储在TaskManagers 的内存中，checkpoints 存储在JobManager 的内存中。</p>

<p>为了更好的持久化大量state，Flink 支持在其他状态后端中存储state，可以通过<code>StreamExecutionEnvironment.setStateBackend(…)</code>来配置。</p>

<hr />

<h2>Queryable State(1.10 Beta)</h2>

<p>该特性可以将Flink 的manager keyed state 暴露在外面，并允许用户在外面查询一个Flink job 的state。</p>

<p>在某些场景下，queryable state 消除了与外部系统（如key-value 存储系统）进行分布式操作/事务的需要，而这经常是实践中瓶颈之处。</p>

<p>另外该特性可能对于debugging 特别有用。</p>

<h3>架构</h3>

<p>Queryable State 特性由三部分组成：</p>

<ul>
	<li>QueryableStateClient：在Flink 集群外运行，负责提交用户的queries；</li>
	<li>QueryableStateClientProxy：在Flink 集群的每一个TaskManager 上运行，负责接收client 的queries，从负责的TaskManager 获取请求的状态并返回给client；</li>
	<li>QueryableStateServer：在Flink 集群的每一个TaskManager 上运行，负责为本地存储state 提供服务</li>
</ul>

<p>Client 连接其中一个proxies，并发送一个与指定key 相关联的state 请求。</p>

<p>keyed state 被组织为Key Groups，每一个TaskManager 分配一个或多个Key Groups。为了寻找哪一个TaskManager 负责持有请求的key 的Key Groups，proxy 需要询问JobManager。</p>

<p>根据JobManager 的回复，proxy 会在TaskManager 上的QueryableStateServer 查询state，并将相应返回给client。</p>

</body>
</html>

