<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<link rel="stylesheet" type="text/css" href="css/style.css" />
		<title>数仓建设案例</title>
	</head>
<body>
<h1>数仓建设案例</h1>

<hr />

<h2>小米mysql实时同步到数据仓库的架构与实践</h2>

<h3>binlog讲解</h3>

<blockquote>
<p>mysql主从复制时使用的binlog日志，记录了所有的DDL和DML语句（除了数据查询语句select、show等），以事件形式记录，还包含语句所执行的消耗时间。</p>
</blockquote>

<p><strong>主从复制原理</strong>：</p>

<ol>
	<li>master在每次准备提交事务完成数据更新前，将改变记录到二进制日志中</li>
	<li>slave发起连接，连接到master，请求获取指定位置的binlog</li>
	<li>master创建dump线程，推送binlog到slave</li>
	<li>slave启动一个I/O线程来读取主库上binlog中的事件，并记录到slave自己的中继日志（relay log）中</li>
	<li>slave还会启动一个sql线程，该线程从relay log中读取事件并在备库中执行，完成数据同步</li>
	<li>slave记录自己的binlog</li>
</ol>

<h3>LCSBinlog</h3>

<h4>架构</h4>

<h5>角色</h5>

<p>Master：主要负责作业的调度</p>

<p>Worker：主要完成具体的数据同步任务</p>

<h5>Worker上运行的作业</h5>

<ol>
	<li>BinlogSyncJob：每一个mysql库都会对应这样一个job，将binlog日志完整地写入到服务创建的Topic中</li>
	<li>MysqlSyncJob：同步历史数据，消费binglog数据，过滤特定库表数据实时同步至用户配置的topic中</li>
</ol>

<h4>同步服务状态</h4>

<p>服务整体依赖于Zookeeper来同步服务状态，记录作业调度信息和标记作业运行状态，在kudu表中记录作业同步进度</p>

<figure><img src="/Users/xerxes/note/dataWareHouse/src/LCSBinlog%E6%9C%8D%E5%8A%A1%E7%8A%B6%E6%80%81%E5%90%8C%E6%AD%A5%E6%8E%A7%E5%88%B6.jpg"/></figure>

<p><strong>控制流程</strong>：</p>

<ol>
	<li>Worker节点通过Zookeeper上注册告知自己可以被调度</li>
	<li>通过在Zookeeper上抢占EPHEMERAL临时节点实现Master的HA</li>
	<li>用户在web上注册BinlogSource同步任务</li>
	<li>Master周期性从配置服务读取Binlog同步作业配置</li>
	<li>Master节点根据Zookeeper伤的调度信息启动新分配任务，停止配置失效任务；作业启动后完成数据实时同步并周期性将同步进度记录到kudu中</li>
	<li>服务上报监控信息到Falcon平台，作业异常退出发送报警邮件</li>
</ol>

<h4>如何保证数据正确性</h4>

<h5>顺序性</h5>

<p>用户配置的每一个BinlogSource都会绑定一个topic，在进行消费的时候需要保证一条mysql记录操作的顺序性，消息队列Talos无法保证全局消息有序，只能保证partition内部有序。</p>

<p>对于配置分库分表或者多库同步任务的BinlogSource，服务根据库表信息进行hash，将数据写入相应的partition，保证同一张表的数据有序。</p>

<h4>一致性</h4>

<p>如何保证在作业异常退出后，作业重新启动能够完整地将mysql中的数据同步到下游系统，主要依赖三点：</p>

<ul>
	<li>服务记录作业同步的offset，重启后从上次commit的offset继续消费</li>
	<li>Binlog数据的顺序性保证了即便数据被重复消费，也能对同一条记录的操作以相同的顺序执行</li>
	<li>下游存储系统基于主键的操作能够保证binlog重复回放后数据最终一致性</li>
</ul>

<hr />

<hr />

<hr />

<h2>美团MySQL实时同步到数据仓库架构与实践</h2>

<p><a href="https://mp.weixin.qq.com/s/GhBPkkW6Qucv0iARHvJ-Rg">[] https://mp.weixin.qq.com/s/GhBPkkW6Qucv0iARHvJ-Rg </a></p>

<p>美团的ODS层数据从MySQL等关系型数据库的业务数据进行采集，导入到Hive中。</p>

<p>过去导入的流程是直连MySQL去select表中的数据，存到本地文件作为中间存储，最后把文件Load到Hive表中。这样做有几个缺点：</p>

<ul>
	<li>随着业务规模的增长，select from mysql -&gt; save to localfile -&gt; load to hive这种数据流花费的时间越来越久，无法满足下游数仓生产的时间要求；</li>
	<li>从MySQL中select大量数据会对MySQL造成较大影响；</li>
	<li>由于Hive本身语法不支持更新、删除等sql原语，对于MySQL中发生update/delete的数据无法很好的进行支持</li>
</ul>

<p>因此，方案转向：CDC + Merge的解决方案，也就是：实时Binlog采集 + 离线处理Binlog还原业务数据这样一套解决方案。</p>

<h3>整体架构</h3>

<figure><img src="/Users/xerxes/note/dataWareHouse/src/%E7%BE%8E%E5%9B%A2binlog%E5%90%8C%E6%AD%A5%E6%9E%B6%E6%9E%84.jpg"/></figure>

<p>Binlog实时采集方面，采用了阿里的开源项目Canal，负责从MySQL实时拉取Binlog并完成适当解析。</p>

<p>Binlog采集后会暂存到Kafka上供下游消费。</p>

<p>离线处理部分，通过以下步骤在Hive上还原一张MySQL表：</p>

<ol>
	<li>采用Linkedin的开源项目Camus，负责每小时把Kafka上的Binlog数据拉取到Hive；</li>
	<li>对每张ODS表，首先需要一次性制作快照，把MySQL里的存量数据读取到Hive上，这一过程底层采用直连MySQL去select数据的方式；</li>
	<li>对每张ODS表，每天基于存量数据和当天增量产生的Binlog做Merge，从而还原出业务数据</li>
</ol>

<p>这样做的好处：</p>

<ul>
	<li>Binlog流式产生，把部分数据处理需求从每天一次的批处理分摊到实时流上，提高性能，减轻对MySQL的压力；</li>
	<li>Binlog本身记录了数据变更的类型，通过一些语义方面的处理，精准还原数据</li>
</ul>

<h3>Binlog实时采集</h3>

<p>Binlog实时采集包括两个主要模块：</p>

<ul>
	<li>CanalManager，主要负责采集任务的分配，监控报警、元数据管理和与外部依赖系统的对接；</li>
	<li>真正执行采集任务的Canal和CanalClient</li>
</ul>

<figure><img src="/Users/xerxes/note/dataWareHouse/src/%E7%BE%8E%E5%9B%A2Binlog%E5%AE%9E%E6%97%B6%E9%87%87%E9%9B%86%E6%B5%81%E7%A8%8B%E7%9A%84%E5%89%AF%E6%9C%AC.jpg"/></figure>

<ol>
	<li>用户提交某个DB的Binlog采集请求；</li>
	<li>CanalManager调用DBA平台相关接口获取这个DB再MySQL实例的相关信息，目的是从中选出最适合Binlog采集的机器；</li>
	<li>CanalManager考虑负载均衡、跨机房传输等因素，把采集实例Canal Instance分发到合适的Canal服务器，即Canal Server上；</li>
	<li>Canal Server收到采集请求，在zk上对收集信息进行注册，注册内容包括：

		<ul>
			<li>以Instance名称命名的永久节点；</li>
			<li>在该永久节点下注册以自身ip:port命名的临时节点</li>
		</ul>

		<p>这样做有两个目的：</p>

		<ul>
			<li>高可用：高可用：CanalManager对Instance进行分发时，会选择两台CanalServer，一台是Running节点，另一台作为Standby节点。Standby节点会对该Instance进行监听，当Running节点出现故障后，临时节点消失，然后Standby节点进行抢占。这样就达到了容灾的目的；</li>
			<li>与CanalClient交互：CanalClient检测到自己负责的Instance所在的Running CanalServer后，便会进行连接，从而接收到CanalServer发来的Binlog数据</li>
		</ul></li>
</ol>

<p>对Binlog的订阅以MySQL的DB为粒度，一个DB的Binlog对应了一个Kafka Topic。底层实现时，一个MySQL实例下所有订阅的DB，都由同一个Canal Instance进行处理。这是因为Binlog的产生是以MySQL实例为粒度的。CanalServer会抛弃掉未订阅的Binlog数据，然后CanalClient将接收到的Binlog按DB粒度分发到Kafka上。</p>

<h3>离线还原MySQL数据</h3>

<p>完成Binlog采集，下一步就是把Binlog从Kafka同步到Hive，利用Binlog还原业务数据。</p>

<figure><img src="/Users/xerxes/note/dataWareHouse/src/%E7%BE%8E%E5%9B%A2Binlog%E7%A6%BB%E7%BA%BF%E8%BF%98%E5%8E%9FMySQL%E6%95%B0%E6%8D%AE.jpg"/></figure>

<h4>Kafka2Hive</h4>

<p>整个Kafka2Hive任务的管理，在美团数据平台的ETL框架下进行，包括任务原语的表达和调度机制等，都同其他ETL类似。</p>

<p>底层采用Camus，并进行二次开发。</p>

<h4>Merge</h4>

<p>Merge流程做了两件事</p>

<ol>
	<li>首先把当天生成的Binlog数据存放到Delta表中，然后和已有的存量数据做一个基于主键的Merge；</li>
	<li>Delta表中的数据是当天的最新数据，当一条数据在一天内发生多次变更时，Delta表中只存储最后一次变更后的数据</li>
</ol>

<hr />

<hr />

<hr />

<h2>马蜂窝实时计算平台MES</h2>

<p><a href="https://mp.weixin.qq.com/s/JMxTaMd8pJOALH78txJAfQ">[https://mp.weixin.qq.com/s/JMxTaMd8pJOALH78txJAfQ] </a></p>

<h3>整体架构设计</h3>

<p>对照Lambda架构，选用Kafka作为消息中间件，批处理层选择Hive、Presto，实时处理层选择Spark、Flink等。</p>

<figure><img src="/Users/xerxes/note/dataWareHouse/src/%E9%A9%AC%E8%9C%82%E7%AA%9DMES%E6%9E%B6%E6%9E%84.jpg"/></figure>

<p>数据从Kafka出来后走两条线：</p>

<ul>
	<li>Spark Streaming，支持秒级别的实时数据，计算结果存入Redis。第二天凌晨，Redis中前一天的所有数据Batch到HBase中；</li>
	<li>Flink+Druid，处理分钟级和小时级的数据</li>
</ul>

<p>入库后往上提供一层Restful API/Thrift API封装，供MES页面或其他业务通过接口的方式获取数据。</p>

<p>如果实时数据出了问题，通过HDFS中的离线主表进行重算，也是两条路径：</p>

<ul>
	<li>为用户服务的MES重算系统：用户自主化选取重算规则，提交重算任务到PrestoSQL集群，计算结果最终落地到HBase，重算后MES的历史数据就会和离线数据算出来的数据保持一致；</li>
	<li>Spark全量重算：由数据平台开发人员内部使用，解决的是基于所有事件组、所有规则的全天数据重算。Spark读取配置规则，重算所有前一天的数据后入库到HBase，保持实时数据和离线数据的一致性</li>
</ul>

<p>监控系统使用Grafana，开放了通用接口给Python、Java等来上报相关信息，只要按照接口上报想要关注的指标并进行简单配置，就可以查询结果，比如MES的延迟时间、一些Restful接口的调用量等，如果出现不正常就告警。</p>

<h4>实时计算引擎</h4>

<h5>技术选型</h5>

<p>组件技术选型的时候在满足自己业务现状的同时，还需要从几个方面考虑：</p>

<ul>
	<li>开源组件是否能覆盖需求</li>
	<li>开源组件的扩展性和二次开发的难度</li>
	<li>开源组件API是否稳定</li>
	<li>开源组件是否有应用于生产环境的案例，比如多少公司应用于生产环境</li>
	<li>开源组件社区是否活跃</li>
	<li>开源组件License限定问题</li>
	<li>开源组件之间的耦合问题</li>
</ul>

<h6>Storm</h6>

<p>第一代流式计算引擎，实现了一个数据流（Data Flow）的模型。可以把它想象成一个发射点，一条一条产生数据，形成的数据流分布式地在集群上按照Bolt的计算逻辑进行转换，完成计算、过滤等操作，在下游实现聚合。</p>

<p>优点是实时性好，可以达到毫秒级。但是吞吐量欠佳，并且只能为消息提供‘至少一次’的处理机制，这意味着可以保证每条消息都能被处理，但也可能发生重复。</p>

<h6>Spark Streaming</h6>

<p>在处理前按时间间隔预先将数据切分成一段一段，进行‘微批次’处理作业。</p>

<p>吞吐性高，实时性虽然不如Storm，但也能达到秒级。在流式语义方面，由于Spark Streaming容错机制基于RDD，依靠CheckPoint，出错之后从该位置重新计算，不会导致重复计算。也可以自己来管理offset，保证Exactly Once的处理。</p>

<h6>Flink</h6>

<p>新一代流式计算引擎，原生的流处理系统，把所有的数据都看成流，认为批处理是流处理中的一种特殊情况。数据基于Flink Stream Source流入，中间经过Operator，从Sink流出。</p>

<p>为了解决容错问题，Flink运用了<strong>分布式快照</strong>的设计与可部分重发的数据源实现容错。用户可以自定义对整个Job进行快照的时间间隔。当任务失败时，Flink将整个Job恢复到最近一次快照，并从数据源重发快照之后的数据。</p>

<p>Flink同时保证了实时性和吞吐量，流式语义也能够保证Exactly Once。</p>

<hr />

<hr />

<hr />

<h2>马蜂窝数据仓库与数据中台</h2>

<p><a href="https://juejin.im/post/5d9c2ffaf265da5b7525a78a">https://juejin.im/post/5d9c2ffaf265da5b7525a78a</a></p>

<h3>数据中台</h3>

<p>数据中台的概念接近<strong>传统数据仓库+大数据平台</strong>的结合体。它是在企业的数据建设经历了数据中心、数据仓库等积累之后，借助平台化的思想，将数据更好的整合与统一，以组件化的方式实现灵活的数据加工与应用，将数据与应用解耦，以服务的方式更好地释放数据价值。</p>

<p>数据平台更多的是体现一种管理思路和架构组织上的变革。</p>

<figure><img src="/Users/xerxes/note/dataWareHouse/src/%E9%A9%AC%E8%9C%82%E7%AA%9D%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%9E%B6%E6%9E%84.jpg"/></figure>

<h3>数仓模型设计</h3>

<h4>建模方法选择</h4>

<p>大数据环境下，业务系统数据体系庞杂，数据结构多样、变更频繁，并且需要快速响应各种复杂的业务需求，传统建模理论无法满足现代数仓需求，需要采取<strong>以需求驱动为主、数据驱动为辅的混合模型设计方法，根据不同的数据层次选择模型</strong>。主要从四个方面综合考虑：</p>

<ul>
	<li>面向主题：采用范式模型理论中的主题划分方法对业务数据进行分类</li>
	<li>一致性保证：采用维度模型理论中的总线结构思想，建立统一的一致性维度表和一致性事实表来保证一致性</li>
	<li>数据质量保证：综合两种理论保证数据质量</li>
	<li>效率保证：合理采用维度退化、变化维、增加冗余等方法，保证数据的计算和查询效率</li>
</ul>

$$ 范式模型 + 维度模型 + 宽表模型 = DW模型 $$
<p>ODS选择贴源的范式模型，不做进一步模型抽象，从节省存储角度考虑对该层采取拉链处理；</p>

<p>DWD和DWS基于对构建成本、性能，易用性角度的考虑，主要采取维度模型和一些款表模型；宽表模型的本质是基于维度模型的扩展，对整个业务以及全节点信息进行垂直与水平方式整合；</p>

<p>同时采取退化维度的方式，将不同维度的度量放入数据表的不同列中，实现业务全流程视图的构建，来提升宽表模型的易用性、查询效率，且易于模型的扩展。</p>

<h4>设计目标</h4>

<ul>
	<li>准确性：数据质量管控要在建模过程中落地，为数据准确性保驾护航</li>
	<li>易用性：兼顾模型的可扩展性和可理解性</li>
	<li>及时性：充分考虑模型的使用效率，提供方便快捷的数据查询和数据计算服务</li>
</ul>

<h4>设计流程</h4>

<figure><img src="/Users/xerxes/note/dataWareHouse/src/%E9%A9%AC%E8%9C%82%E7%AA%9D%E6%95%B0%E4%BB%93%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B.jpg"/></figure>

<h5>需求调研</h5>

<p>收集和理解业务方需求，就特定需求的口径达成一致，在对需求中涉及到的业务系统或系统模块所承担的功能进行梳理后进行表字段级分析，并对数据进行验证，确保现有数据能够支持业务需求。</p>

<h5>模型设计</h5>

<p>根据需求和业务调研结果对模型进行初步归类，选择合适的主题域进行模型存放；</p>

<p>确定主题后进入数据模型的设计阶段，逻辑模型设计过程要考虑总线结构构建、模型规范定义等问题；</p>

<p>物理模型设计以逻辑模型为基础，兼顾存储性能等因素对逻辑模型做的物理化的过程，是逻辑模型的最终物理实现。</p>

<p>物理模型一般与逻辑模型保持一致，模型设计完成后需要进入评审与Mapping设计。</p>

<h5>模型开发</h5>

<p>就是对模型计算脚本的代码实现过程，其中包含了数据映射、脚本实现、测试验证等开发过程。</p>

<p>单元测试完成后需要通知业务方一起对模型数据进行业务验证，对验证问题做收集，返回验证模型设计的合理性。</p>

<h5>模型上线</h5>

<p>完成验证后的模型就可以在线上生产环境进行部署。上线后需要为模型配置监控，及时掌握为业务提供数据服务的情况。</p>

<p>还要将模型的实体和属性说明文档发布给数据仓库使用者。</p>

</body>
</html>

