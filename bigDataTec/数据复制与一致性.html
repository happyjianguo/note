<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<link rel="stylesheet" type="text/css" href="css/style.css" />
		<title>数据复制与一致性</title>
	</head>
<body>
<h1>数据复制与一致性</h1>

<p>分布式系统中，将一份数据复制成多份除了增加存储系统高可用性外还可以增加读操作的并发性，但是在并发的众多客户端读/写请求下，如何维护数据一致视图非常重要。</p>

<hr />

<h2>基本原则与设计理念</h2>

<h3>原教旨CAP主义</h3>

<blockquote>
<p>CAP是对Consistency/Availability/Partition Tolerance 的一种简称，分别代表：强一致性、可用性和分区容忍性。</p>
</blockquote>

<ul>
	<li>强一致性：在分布式系统中统一数据多副本情形下，对于数据的更新操作体现出的效果与只有单份数据是一样的；</li>
	<li>可用性：客户端在任何时刻对大规模数据系统的读/写操作都应该保证在有限定延时内完成；</li>
	<li>分区容忍性：在大规模分布式数据系统中，网络分区现象，即分区间的机器无法进行网路通信的情况必然会发生，所以系统应该能够在这种情况下仍然继续工作</li>
</ul>

<p>CAP最初由Eric Brewer于1999年提出，同时他也证明了：对于一个大规模分布式数据系统来说，CAP三要素最多只能实现其中两个并放宽第三个要素来保证其他两个要素被满足，这就是CAP原则的精髓所在。一般在网络情况下，运行环境出现网络分区是不可避免的，所以系统必须具备分区容忍性，于是一般在此场景下设计大规模分布式系统时，架构师往往在AP和CP中进行权衡和选择。</p>

<h3>CAP重装上阵（CAP Reloaded）</h3>

<p>Eric Brewer在2012年指出：实践过程中应用CAP理论时不得不在三要素中选择两个而牺牲另外一个的做法具有误导性，具体体现在几方面：</p>

<ul>
	<li>在实际系统中，网路分区出现的概率很小，并不应该为了容忍这种小概率事件而在设计之初就选择放弃A或者C；</li>
	<li>其次，即使是必须在AC之间做出取舍的时候，也不应该是粗粒度地在整个系统级别进行取舍，而是应该考虑系统中存在不同的子系统，甚至是在不同的系统运行时或者在不同的数据间进行灵活的差异化的细粒度取舍；</li>
	<li>再次，CAP三者并非是绝对二元式地有或者没有，而是应该将其看作连续变量，即可以看作在一定程度上的有或者没有。比如可用性指标中延时长度多少算可用这都是可以看作连续变量的。在实际进行取舍时并非加以完全舍弃，而是可以考虑在多大程度上进行取舍</li>
</ul>

<p>考虑到以上实际应用CAP理论时的误导性，采取如下应用CAP策略：在绝大多数系统未产生网络分区的情形下尽可能保证CAP三者兼得；发生网络分区时，系统应该能够识别这种状况并对其进行正确处理，具体分三个步骤：首先能够识别网络分区发生；然后在网络分区场景下进入明确的分区模式，此时可能会限制某些系统操作；最后在网络分区解决后能够进行善后处理，即恢复数据的一致性或者弥补分区模式中产生的错误</p>

<h3>ACID原则</h3>

<ul>
	<li>原子性（Atomicity）：指一个事务要么全部执行，要么完全不执行；</li>
	<li>一致性（Consistency）：系统从一个正确的状态,迁移到另一个正确的状态。正确是指事务开始和结束时，应该始终满足一致性约束条件；</li>
	<li>事务独立（Isolation）：如果有多个事务同时执行，彼此不需要知道对方的存在，执行时不会互相影响；</li>
	<li>持久性（Durability）：事务运行成功后，对系统状态的更新是永久的</li>
</ul>

<h3>BASE原则</h3>

<ul>
	<li>基本可用（Basically Available）：在绝大多数时间内系统处于可用状态，允许偶尔失败；</li>
	<li>软状态或者柔性状态（Soft State）：是指数据状态不要求在任一时刻都完全保持一致。软状态可理解为处于有状态（State）和无状态（Stateless）之间的中间状态；</li>
	<li>最终一致性（Eventual Consistency）：一种弱一致性，要求在给定时间窗口内数据会到达一致状态</li>
</ul>

<p>关系型数据库采纳ACID原则获得高可靠性和强一致性，而大多数大数据环境下的云存储系统和NoSQL系统则采取BASE原则，这种原则与ACID原则差异很大，BASE通过牺牲强一致性来获得高可用性。</p>

<p>目前NoSQL系统与云存储系统的发展过程正向逐步提供局部ACID特性发展，即从全局而言符合BASE原则，但是从局部支持ACID原则，在两者之间建立平衡。</p>

<h3>CAP/ACID/BASE三者的关系</h3>

<p>ACID和BASE原则是在明确提出CAP理论之前关于如何对待可用性和强一致性的两种完全不同的设计哲学。ACID强调数据一致性，BASE强调可用性。</p>

<p>CAP和ACID的明显差异包括：</p>

<ul>
	<li>ACID的一致性指的是操作的一致性，CAP的一致性指的是数据的一致性；</li>
	<li>当出现网络分区时，ACID中的事务独立只能在多分区中的某个分区执行，因为事务的序列化要求通信；</li>
	<li>当出现网络分区时，多个分区都可以各自进行ACID中的数据持久化操作，网络分区解决后，如果每个分区都提供持久化记录，则系统可用根据这些记录发现违反ACID一致性约束的内容并给予修正</li>
</ul>

<h3>幂等性</h3>

<p>幂等性是分布式系统中很重要的概念，对理解很多分布式系统的设计思路有很大帮助。</p>

<p>在抽象代数里也存在幂等概念，对于一元运算来说，满足f(f(x))=f(x)条件的运算即可成为满足幂等性，比如绝对值运算。对于二元运算，如果满足f(x,x)=x条件的运算也可以成为满足幂等性，比如实数集和运算max(x,x)=x。</p>

<p>分布式系统中的幂等性是指：<strong>调用方反复执行统一操作与只正确执行一次操作效果相同，即对分布式系统内部状态来说，同一操作调用一次与反复调用多次其状态保持一致</strong>。</p>

<p>在分布式环境下，系统具有幂等性很重要，因为分布式环境下调用方和被调用方往往需要通过网络通信，如果调用方已经正确调用服务方提供的功能，但是由于网络故障，调用方并未收到调用成功的响应，会认为调用失败从而再次调用相同操作。</p>

<hr />

<h2>副本更新策略</h2>

<p>一般大规模分布式存储系统会将一份数据在系统内复制多份并放置在不同机器存储，对于数据更新操作会存在潜在的数据一致性问题。</p>

<p>副本更新策略有三种：同时更新策略、主从式更新策略及任意节点更新策略。</p>

<h3>同时更新</h3>

<p>多副本同时更新有以下两种情形：</p>

<ul>
	<li>类型A

		<p>不通过任何一致性协议直接同时更新多个副本数据。</p>

		<p>潜在问题：假设同一时刻两个不同客户端对这个数据同时发出两个更新请求，那么系统无法确定其执行先后顺序</p></li>
	<li>类型B

		<p>通过某种一致性协议预先处理，一致性协议用来确定不同更新操作的执行顺序，这样可以保证数据一致性，但是由于一致性协议有处理成本，所以请求延时会有所增加</p></li>
</ul>

<h3>主从式更新</h3>

<p>如果某数据的多副本中存在一个主副本，其他副本为从副本，则可称为主从式更新策略。</p>

<p>所有对这个数据的更新操作首先提交到主副本，再由主副本通知从副本数据更新，如果同时产生多个数据更新操作，主副本决定不同更新操作的顺序，所有从副本也遵循主副本的更新顺序。</p>

<ul>
	<li>类型A：同步方式

		<p>主副本等待所有从副本更新完成之后才确认更新操作完成，这可以确保数据的一致性，但是存在请求延时。尤其在多副本跨数据中心的情形下，因为请求延时取决于最慢的那个副本的更新速度</p></li>
	<li>类型B：异步方式

		<p>主副本在通知从副本更新之前即可确认更新操作。</p>

		<p>这种场景下，假设主副本还没有通知任何其他从副本就发生崩溃，那么数据一致性可能会出现问题，所以一般会首先在另外的可靠存储位置将这次更新操作记录下来，以防止这种情况发生。</p>

		<p>这种异步方式的请求延时和一致性之间的权衡取决于该读操作的响应方式，这又可以分为以下两种情形：</p>

		<ul>
			<li>所有读请求都需要通过主副本来响应，即任意一个副本接收到读请求后将其转发给主副本：这样数据的强一致性可以保证，但是会增加请求延时</li>
			<li>任意一个副本都可以响应读请求：请求延时会大大降低，但是会导致读结果不一致</li>
		</ul></li>
	<li>类型C：混合方式

		<p>也可以采取同步异步混合的方式，即主副本首先同步更新部分从副本数据，然后即可确认更新操作完成，其他副本通过异步方式获得更新。</p>

		<p>消息系统Kafka在维护数据副本一致性时即采取此种混合方式。这种情形下请求延时和一致性之间的权衡也取决于以下两种读操作的响应方式：</p>

		<ul>
			<li>读操作的数据至少要从一个同步更新的节点中读出：强一致性可以获得保证，但是请求延时会增加。但是由于其涉及的节点都少于单一的同步和异步方式，所以请求延时问题相较前两种没那么严重</li>
			<li>读操作不要求一定要从至少一个同步更新节点中读出：存在不一致问题</li>
		</ul></li>
</ul>

<h3>任意节点更新</h3>

<p>数据更新请求可能发给多副本的任意一个节点，然后由这个节点来负责通则通知其他副本进行数据更新。</p>

<p>与“主从式更新”的区别：对于某个数据，并不存在固定哪个首先响应更新操作的主副本，而是任意一个节点都可以响应。所以其特殊性在于：有可能有两个不同客户端在同一时刻对同一个数据发出数据更新请求，而此时有可能有两个不同副本各自响应。</p>

<p>这种更新方式，请求延时和一致性的权衡有以下两种情形：</p>

<ul>
	<li>类型A：同步通知其他副本

		<p>存在“主从式更新”类型A相似的情况。除此之外，为了识别出是否存在客户端同时更新不同副本的情况，还需要额外付出更多请求延时</p></li>
	<li>类型B：异步通知其他副本

		<p>存在“同时更新”策略及“主从式更新”类型B类似的问题。</p></li>
</ul>

<hr />

<h2>分布式系统常见同步机制</h2>

<p>分布式系统为保证数据高可用，需要为数据保存多个副本，随之而来的问题是如何在不同副本间同步数据。</p>

<h3>常见机制</h3>

<figure><img src="/Users/xerxes/note/bigDataTec/src/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%B8%B8%E8%A7%81%E5%90%8C%E6%AD%A5%E6%9C%BA%E5%88%B6.jpg"/></figure>

<ul>
	<li>backup：定期备份，对现有的系统的性能基本没有影响，但节点宕机时只能勉强恢复</li>
	<li>Master-Slave：主从复制，异步复制每个指令，可以看作是粒度更细的定期备份</li>
	<li>Multi-Master：多主，MS的加强版，可以在多个节点上写，事后再想办法同步</li>
	<li>2Phase-Commit：两阶段提交，同步先确保通知到所有节点再写入，性能容易卡在主节点上</li>
	<li>Paxos：类似2PC，同一时刻有多个节点可以写入，只需要通知大多数节点，有更高的吞吐</li>
</ul>

</body>
</html>

