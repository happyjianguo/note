<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<link rel="stylesheet" type="text/css" href="css/style.css" />
		<title>数据复制与一致性</title>
	</head>
<body>
<h1>数据复制与一致性</h1>

<p>分布式系统中，将一份数据复制成多份除了增加存储系统高可用性外还可以增加读操作的并发性，但是在并发的众多客户端读/写请求下，如何维护数据一致视图非常重要。</p>

<hr />

<h2>基本原则与设计理念</h2>

<h3>原教旨CAP主义</h3>

<blockquote>
<p>CAP是对Consistency/Availability/Partition Tolerance 的一种简称，分别代表：强一致性、可用性和分区容忍性。</p>
</blockquote>

<ul>
	<li>强一致性：在分布式系统中统一数据多副本情形下，对于数据的更新操作体现出的效果与只有单份数据是一样的</li>
	<li>可用性：客户端在任何时刻对大规模数据系统的读/写操作都应该保证在有限定延时内完成</li>
	<li>分区容忍性：在大规模分布式数据系统中，网络分区现象，即分区间的机器无法进行网路通信的情况必然会发生，所以系统应该能够在这种情况下仍然继续工作</li>
</ul>

<p>CAP最初由Eric Brewer于1999年提出，同时他也证明了：对于一个大规模分布式数据系统来说，CAP三要素最多只能实现其中两个并放宽第三个要素来保证其他两个要素被满足，这就是CAP原则的精髓所在。一般在网络情况下，运行环境出现网络分区是不可避免的，所以系统必须具备分区容忍性，于是一般在此场景下设计大规模分布式系统时，架构师往往在AP和CP中进行权衡和选择。</p>

<h3>CAP重装上阵（CAP Reloaded）</h3>

<p>Eric Brewer在2012年指出：实践过程中应用CAP理论时不得不在三要素中选择两个而牺牲另外一个的做法具有误导性，具体体现在几方面：</p>

<ul>
	<li>在实际系统中，网路分区出现的概率很小，并不应该为了容忍这种小概率事件而在设计之初就选择放弃A或者C</li>
	<li>其次，即使是必须在AC之间做出取舍的时候，也不应该是粗粒度地在整个系统级别进行取舍，而是应该考虑系统中存在不同的子系统，甚至是在不同的系统运行时或者在不同的数据间进行灵活的差异化的细粒度取舍</li>
	<li>再次，CAP三者并非是绝对二元式地有或者没有，而是应该将其看作连续变量，即可以看作在一定程度上的有或者没有。比如可用性指标中延时长度多少算可用这都是可以看作连续变量的。在实际进行取舍时并非加以完全舍弃，而是可以考虑在多大程度上进行取舍</li>
</ul>

<p>考虑到以上实际应用CAP理论时的误导性，采取如下应用CAP策略：在绝大多数系统未产生网络分区的情形下尽可能保证CAP三者兼得；发生网络分区时，系统应该能够识别这种状况并对其进行正确处理，具体分三个步骤：首先能够识别网络分区发生；然后在网络分区场景下进入明确的分区模式，此时可能会限制某些系统操作；最后在网络分区解决后能够进行善后处理，即恢复数据的一致性或者弥补分区模式中产生的错误</p>

<h3>ACID原则</h3>

<ul>
	<li>原子性（Atomicity）：指一个事务要么全部执行，要么完全不执行</li>
	<li>一致性（Consistency）：事务开始和结束时，应该始终满足一致性约束条件</li>
	<li>事务独立（Isolation）：如果有多个事务同时执行，彼此不需要知道对方的存在，执行时不会互相影响</li>
	<li>持久性（Durability）：事务运行成功后，对系统状态的更新是永久的</li>
</ul>

<h3>BASE原则</h3>

<ul>
	<li>基本可用（Basically Available）：在绝大多数时间内系统处于可用状态，允许偶尔失败</li>
	<li>软状态或者柔性状态（Soft State）：是指数据状态不要求在任一时刻都完全保持一致。软状态可理解为处于有状态（State）和无状态（Stateless）之间的中间状态</li>
	<li>最终一致性（Eventual Consistency）：一种弱一致性，要求在给定时间窗口内数据会到达一致状态</li>
</ul>

<p>关系型数据库采纳ACID原则获得高可靠性和强一致性，而大多数大数据环境下的云存储系统和NoSQL系统则采取BASE原则，这种原则与ACID原则差异很大，BASE通过牺牲强一致性来获得高可用性。</p>

<p>目前NoSQL系统与云存储系统的发展过程正向逐步提供局部ACID特性发展，即从全局而言符合BASE原则，但是从局部支持ACID原则，在两者之间建立平衡。</p>

<h3>CAP/ACID/BASE三者的关系</h3>

<p>ACID和BASE原则是在明确提出CAP理论之前关于如何对待可用性和强一致性的两种完全不同的设计哲学。ACID强调数据一致性，BASE强调可用性。</p>

<p>CAP和ACID的明显差异包括：</p>

<ul>
	<li>ACID的一致性指的是操作的一致性，CAP的一致性指的是数据的一致性</li>
	<li>当出现网络分区时，ACID中的事务独立只能在多分区中的某个分区执行，因为事务的序列化要求通信</li>
	<li>当出现网络分区时，多个分区都可以各自进行ACID中的数据持久化操作，网络分区解决后，如果每个分区都提供持久化记录，则系统可用根据这些记录发现违反ACID一致性约束的内容并给予修正</li>
</ul>

<h3>幂等性</h3>

<p>幂等性是分布式系统中很重要的概念，对理解很多分布式系统的设计思路有很大帮助。</p>

<p>在抽象代数里也存在幂等概念，对于一元运算来说，满足f(f(x))=f(x)条件的运算即可成为满足幂等性，比如绝对值运算。对于二元运算，如果满足f(x,x)=x条件的运算也可以成为满足幂等性，比如实数集和运算max(x,x)=x。</p>

<blockquote>
<p>分布式系统中的幂等性是指：调用方反复执行统一操作与只正确执行一次操作效果相同，即对分布式系统内部状态来说，同一操作调用一次与反复调用多次其状态保持一致。</p>
</blockquote>

<p>在分布式环境下，系统具有幂等性很重要，因为分布式环境下调用方和被调用方往往需要通过网络通信，如果调用方已经正确调用服务方提供的功能，但是由于网络故障，调用方并未收到调用成功的响应，会认为调用失败从而再次调用相同操作。</p>

<hr />

<h2>一致性</h2>

<h3>Consensus</h3>

<p>更准确的翻译是<strong>共识，关注的是多个提议者达成共识的过程</strong>。</p>

<p>比如Paxos，Raft等共识算法，本质上解决的是如何在分布式系统中保证所有节点对某个结果达成共识，其中需要考虑节点宕机、网络延迟、网络分区等问题。</p>

<p>共识算法通常应用在复制状态机中，比如etcd，zookeeper，用于构建高可用容错系统。在这种应用场景下，Raft/Paxos共识算法被用来确保各个复制状态机（节点）的日志是一致的。</p>

<p>类似的，共识算法在区块链中也非常重要，但区块链通常用的是POW（Proof of work）或者POS（Proof of stack），这类共识算法通常用在公网，去中心化的情形下。</p>

<h3>Coherence</h3>

<p>Coherence通常出现在Cache Coherence一词中，作为<strong>缓存一致性</strong>被提出。</p>

<p>现代多核CPU Cache都是多层结构，通常每个CPU Core都有一个私有的LB/SB，L1，L2级Cache，多个CPU core共享一个L3级Cache。</p>

<p>当CPU1修改全局变量并写入了L1 Cache中，CPU2想要读取此变量时，就需要Cache Coherence来保证CPU2读取到的是最新变量。<strong>CC的本质是让多组Cache看起来就像一组Cache一样</strong>。</p>

<h3>ACID Consistency</h3>

<p>ACID中的一致性指的是数据库的一致性约束，完全与数据库规则有关，包括约束、级联、触发器等。在事务开始之前和事务结束之后，都必须遵守这些不变量，保证数据库的完整性不被破坏。</p>

<h3>CAP Consistency</h3>

<p>CAP理论中的一致性指的是常见的分布式系统中的一致性，更确切地说是分布式一致性中的一种：线性一致性（Linearizability），也叫做原子一致性（Atomic Consistency）。</p>

<p>理想情况下，希望整个分布式系统中的所有事件都有先后顺序，这种顺序就是全序，即整个事件集合中任意两个事件都有先后顺序。但是1. 物理时间很难同步；2. 网络是异步的。因此分布式系统中，想要维持全序是非常困难的。</p>

<p>不同的节点对两个事件谁先发生可能具有不同的看法，并且大部分时候我只需要知道偏序关系，用于确保因果关系。所谓偏序关系是指:</p>

<ol>
	<li>如果a和b是同一个进程中的事件，并且a在b前面发生，那么 a-&gt;b；</li>
	<li>如果a代表了某个进程的消息发送事件，b代表另一进程中针对这同一个消息的接收事件，那么a-&gt;b；</li>
	<li>如果 a-&gt;b且b-&gt;c，那么a-&gt;c (传递性)</li>
</ol>

<p>逻辑时间中的 Lamport Clock和Vector Clock等都可以用于建立偏序关系。</p>

<h4>一致性模型分类</h4>

<blockquote>
<p>从严格意义上来讲，理想情况下，真正的一致性模型只有一种，即我们常说的<strong>强一致性</strong>。意思是当对某个数据进行了一个更新操作后，所有后续的观察者都应该感知到这次数值变化并以此为基础进行后续的读/写行为。</p>
</blockquote>

<p>但真实情况下，多数NoSQL系统采用的都是弱一致性模型。这是多机分布情况下追求高可用性和高扩展性必须要做的妥协。因为根据CAP理论可知：在有些情况下，没有系统能够同时满足一致性、可用性和分区容忍性。而对于互联网环境下的分布式计算系统，分区容忍性是一个先天设定的场景，存储系统只能从一致性和可用性中选择一个因素来构建具体系统。</p>

<p>一致性模型包括：强一致性、弱一致性、最终一致性、因果一致性、“读你所写”一致性、会话一致性、单调读一致性以及单调写一致性。</p>

<figure><img src="/Users/xerxes/note/bigDataTec/src/%E4%B8%80%E8%87%B4%E6%80%A7%E5%85%B3%E7%B3%BB%E5%9B%BE.jpg"/></figure>

<p>如图可知，最终一致性是弱一致性的一种特殊情况，而除了强一致性之外，其他模型都属于最终一致性模型的特例或者变体。</p>

<h5>强一致性</h5>

<p>也就是线性一致性（Linearizability），也称为原子一致性（atomic consistency）。线性一致性强于顺序一致性，是程序能实现的最高的一致性模型，也是分布式系统用户最期望的一致性。</p>

<p>与顺序一致性相比，线性一致性只多了一条约束：如果事件A开始时间晚于事件B结束时间，则在最终事件历史中，要求B在A前。</p>

<p>对于连接到数据库的所有进程，看到的关于某数据的值是一致的，如果某进程对数据进行了更新，所有进程的后续操作都会以这个更新后的值为基准，直到这个数据被其他进程改变为止。</p>

<p>不能满足强一致性的情形皆可统称为弱一致性。</p>

<h5>最终一致性</h5>

<p>最终一致性是一种弱一致性。它无法保证某个数据做出更新后，所有后续针对这个数据的操作能够立即看到新数据，而是需要一个时间段，时间段内数据也许是不一致的，在这个时间段之后可以保证这一点，这个时间段称为“不一致窗口”。</p>

<p>在分布式环境下，为了达到高可用性，同一份数据通常被存储到多个机器节点。不同进程可能操作数据的不同备份，当某进程对数据做了更新，需要一定时间来将这个新数据传播到数据的所有其他备份中，而这个时间区间就是不一致窗口。</p>

<h5>因果一致性</h5>

<p>在进程之间有因果依赖关系的情形下，当进程A将数据更新后，会通知进程B数据做出了更新，进程B之后的操作会以新数据作为基础进行读/写。即进程A和进程B保持了数据的因果一致性。而对其他进程来说，还是会在不一致窗口中看到数据的旧值。</p>

<h5>“读你所写”一致性</h5>

<p>因果一致性的特例，在概念上可理解为：进程A更新了一条数据，立即给自己发出一条数据发生变化的通知，所以进程A之后的操作都以新数据作为基础。其他进程不受影响，在不一致窗口内仍旧可能会看到x的旧值。</p>

<h5>会话一致性</h5>

<p>“读你所写”一致性的一种现实版本变体即“会话一致性”。当进程A通过会话与数据库系统连接，在同一个会话内，可以保证其“读你所写”一致性。而在不一致窗口内，如果因为系统故障等原因导致会话结束，那么进程A可能读到旧值。</p>

<h5>单调读一致性</h5>

<p>最终一致性的另一种变体。它保证如果某个进程读取到数据的某个版本，那么系统所有后续的读取操作都不能看到比这个版本老的数据。</p>

<h5>单调写一致性</h5>

<p>另外一种最终一致性的另一种变体。对于某进程来说，单调写一致性可以保证其多次写操作的序列化，如果没有这种保证，对于开发者来说很难进行程序开发。</p>

<h5>顺序一致性 Sequential Consistency</h5>

<p>SC最早用来描述多核CPU的行为，其定义：</p>

<blockquote>
<p>the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.</p>

<p>如果可以找到一个所有CPU执行指令的排序，该排序中每个CPU要执行指令的顺序得以保持，且实际的CPU执行结果与该指令排序的结果一致，则称该次执行达到了顺序一致性。</p>
</blockquote>

<p>SC保证了两点：</p>

<ul>
	<li>每个线程内部的指令都是按照程序规定的顺序执行的；</li>
	<li>保证上一点的基础上，线程之间是可以交错执行的</li>
</ul>

<h6>多核处理器的顺序一致性</h6>

<p>CPU执行的主要瓶颈在与内存交互，为了让CPU高速执行，CPU内部使用了多级缓存。多级缓存的存在，可能导致CPU内部顺序执行的命令会不满足顺序一致性：</p>

<figure><img src="/Users/xerxes/note/bigDataTec/src/%E9%A1%BA%E5%BA%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8F%8D%E4%BE%8B.jpg"/></figure>

<p>CPU执行命令时会乱序执行，例如在一些情况下CPU会将数据写入的指令提前执行，因为写入内存是很耗时的。同理，编译器在编译代码时也会重排指令的顺序，以提升整体的性能。</p>

<p>现代硬件体系遵循的其实是<strong>sequential consistency for data race free programs</strong>，即如果程序没有数据竞争，则CPU可以保证顺序一致性，如果遇到数据竞争，就需要在程序中使用数据同步的机制（如加锁）。</p>

<h6>ZooKeeper的顺序一致性</h6>

<p>ZooKeeper的一致性保证第一条是：</p>

<blockquote>
<p>Sequential Consistency : Updates from a client will be applied in the order that they were sent.</p>

<p>顺序一致性：客户端发送的更新指令，服务端会按照它们发送的顺序执行。</p>
</blockquote>

<hr />

<h2>副本更新策略</h2>

<p>一般大规模分布式存储系统会将一份数据在系统内复制多份并放置在不同机器存储，对于数据更新操作会存在潜在的数据一致性问题。</p>

<p>副本更新策略有三种：同时更新策略、主从式更新策略及任意节点更新策略。</p>

<h3>同时更新</h3>

<p>多副本同时更新有以下两种情形：</p>

<ul>
	<li>类型A

		<p>不通过任何一致性协议直接同时更新多个副本数据。</p>

		<p>潜在问题：假设同一时刻两个不同客户端对这个数据同时发出两个更新请求，那么系统无法确定其执行先后顺序</p></li>
	<li>类型B

		<p>通过某种一致性协议预先处理，一致性协议用来确定不同更新操作的执行顺序，这样可以保证数据一致性，但是由于一致性协议有处理成本，所以请求延时会有所增加</p></li>
</ul>

<h3>主从式更新</h3>

<p>如果某数据的多副本中存在一个主副本，其他副本为从副本，则可称为主从式更新策略。</p>

<p>所有对这个数据的更新操作首先提交到主副本，再由主副本通知从副本数据更新，如果同时产生多个数据更新操作，主副本决定不同更新操作的顺序，所有从副本也遵循主副本的更新顺序。</p>

<ul>
	<li>类型A：同步方式

		<p>主副本等待所有从副本更新完成之后才确认更新操作完成，这可以确保数据的一致性，但是存在请求延时。尤其在多副本跨数据中心的情形下，因为请求延时取决于最慢的那个副本的更新速度</p></li>
	<li>类型B：异步方式

		<p>主副本在通知从副本更新之前即可确认更新操作。</p>

		<p>这种场景下，假设主副本还没有通知任何其他从副本就发生崩溃，那么数据一致性可能会出现问题，所以一般会首先在另外的可靠存储位置将这次更新操作记录下来，以防止这种情况发生。</p>

		<p>这种异步方式的请求延时和一致性之间的权衡取决于该读操作的响应方式，这又可以分为以下两种情形：</p>

		<ul>
			<li>所有读请求都需要通过主副本来响应，即任意一个副本接收到读请求后将其转发给主副本：这样数据的强一致性可以保证，但是会增加请求延时</li>
			<li>任意一个副本都可以响应读请求：请求延时会大大降低，但是会导致读结果不一致</li>
		</ul></li>
	<li>类型C：混合方式

		<p>也可以采取同步异步混合的方式，即主副本首先同步更新部分从副本数据，然后即可确认更新操作完成，其他副本通过异步方式获得更新。</p>

		<p>消息系统Kafka在维护数据副本一致性时即采取此种混合方式。这种情形下请求延时和一致性之间的权衡也取决于以下两种读操作的响应方式：</p>

		<ul>
			<li>读操作的数据至少要从一个同步更新的节点中读出：强一致性可以获得保证，但是请求延时会增加。但是由于其涉及的节点都少于单一的同步和异步方式，所以请求延时问题相较前两种没那么严重</li>
			<li>读操作不要求一定要从至少一个同步更新节点中读出：存在不一致问题</li>
		</ul></li>
</ul>

<h3>任意节点更新</h3>

<p>数据更新请求可能发给多副本的任意一个节点，然后由这个节点来负责通则通知其他副本进行数据更新。</p>

<p>与“主从式更新”的区别：对于某个数据，并不存在固定哪个首先响应更新操作的主副本，而是任意一个节点都可以响应。所以其特殊性在于：有可能有两个不同客户端在同一时刻对同一个数据发出数据更新请求，而此时有可能有两个不同副本各自响应。</p>

<p>这种更新方式，请求延时和一致性的权衡有以下两种情形：</p>

<ul>
	<li>类型A：同步通知其他副本

		<p>存在“主从式更新”类型A相似的情况。除此之外，为了识别出是否存在客户端同时更新不同副本的情况，还需要额外付出更多请求延时</p></li>
	<li>类型B：异步通知其他副本

		<p>存在“同时更新”策略及“主从式更新”类型B类似的问题。</p></li>
</ul>

<hr />

<h2>分布式事务的一致性协议</h2>

<h3>Paxos协议</h3>

<h4>副本状态机副本（Replicated State Machines）</h4>

<p>在分布式环境下，一致性协议的应用场景一般会采用副本状态机来表达，这是对各种不同应用场景的一种抽象化表述。</p>

<figure><img src="/Users/xerxes/note/bigDataTec/src/%E5%89%AF%E6%9C%AC%E7%8A%B6%E6%80%81%E6%9C%BA.jpg"/></figure>

<p>集群中多台服务器各自保存一份Log副本及内部状态机，Log内顺序记载客户端发来的操作指令，服务器依次执行Log内的指令并将其体现到内部状态机上，如果保证每台机器内的Log副本内容完全一致，那么对应的状态机也可以保证整体状态一致。一致性协议的作用就是保证各个Log副本数据的一致性，即图中一致性模块起的作用。</p>

<p>某台服务器在接收到客户端的操作指令后，将其追加到自身的Log尾部，然后和其他服务器的一致性模块进行通信，保证其他服务器（即使有服务器发生故障）的Log最终能够以同样的顺序保存同样的操作指令，当操作指令能够正确复制，那么每台服务器按照Log内记录顺序执行操作指令，最终所有服务器的内部状态保持一致，服务器将执行操作命令后的状态结果返回客户端作为操作结果。通过这种方式使得整个集群对于外部客户端看起来就像单机一样。</p>

<p>实际实现副本状态机中的一致性协议时，往往追求几个特性：</p>

<ul>
	<li>安全性：即非拜占庭模式下，状态机从不返回错误的结果，多个提议中只会有一个被选中</li>
	<li>可用性：只要大多数服务器正常，整个服务就保持可用</li>
	<li>一般情况下，大多数状态机维护Log一致即可快速通知客户端操作成功，这样避免了少数最慢的状态机拖慢整个请求响应速度</li>
</ul>

<h4>Paxos基本概念</h4>

<p><strong>Paxos又可以细分为两种</strong>：</p>

<ul>
	<li>单Paxos（Single-Decree Paxos）

		<p>副本状态机中各个服务器针对Log中固定某个位置的操作命令通过协议达成一致，因为可能某一时刻不同服务器的Log中相同位置的操作命令不一样，通过执行协议后使得各个服务器对应某个固定位置的操作命令达成一致</p></li>
	<li>多Paxos（Multi-Paxos）

		<p>指这些服务器对应的Log内容中多个位置的操作命令序列通过协议保持一致。多Paxos往往是同时运行的多个单Paxos协议共同执行的结果。</p></li>
</ul>

<p><strong>并行进程（对应副本状态机上每台服务器的一致性模块）分为三种角色</strong>：</p>

<ul>
	<li>倡议者（Proposer）：倡议者可以提出提议以供投票表决</li>
	<li>接受者（Acceptor）：接受者可以对提议者提出的提议进行投票表决，从众多提议中选出唯一确定的一个</li>
	<li>学习者（Learner）：学习者无倡议投票权，但是可以从接受者那里获知是哪个提议最终被选中</li>
</ul>

<p><strong>异步通信环境下的非拜占庭模型（Non-Byzantine Model）</strong>：</p>

<ul>
	<li>并发进程的行为可以以任意速度执行，允许运行失败，在失败后也许会重启并再次运行</li>
	<li>并发进程之间通过异步方式发送信息通信，通信时间可以任意长，信息可能会在传输过程中丢失，也允许重复发送相同的消息，多重信息的顺序可以任意。但是有一点：信息不允许被篡改（真实分布式计算环境下可以通过内容完整性检测来解决）</li>
</ul>

<p>Paxos协议以及很多一致性协议都是基于非拜占庭模型的，即在非拜占庭条件下Paxos协议可以就不同提议达成一致，而在拜占庭模型下情况会更加复杂</p>

<h4>Paxos一致性协议</h4>

<p>Paxos的目的是在非拜占庭条件下，当多个并行进程提出不同的操作指令时，如何能够达成一致。Paxos协议可以描述为两阶段过程：</p>

<ul>
	<li>阶段一

		<p>1.1【倡议者视角】倡议者选择倡议编号n，然后向大多数（超过半数以上）接受者发送Prepare请求，请求中附带倡议编号n</p>

		<p>1.2【接受者视角】对于某个接受者来说，如果接收到带有倡议编号n的Prepare请求，则做判断：若倡议编号n比此接受者之前响应过的任何其他Prepare请求附带的倡议编号都大，那么此接受者会给倡议者响应，并承诺不会响应之后接收到的其他任何倡议编号小于n的请求；如果接受者曾经响应过2.2阶段的Accept请求，则将所有响应的Accept请求中的倡议编号最高的倡议内容发送给倡议者，倡议内容包括两项信息：Accept请求中的倡议编号以及倡议值。若倡议编号n不比此接受者之前响应过的任何其他Prepare请求附带的倡议编号都大，那么此接受者不会给倡议者响应</p></li>
	<li>阶段二

		<p>2.1【倡议者视角】如果倡议者接收到大多数接受者关于带有倡议编号n的Prepare请求的响应，那么倡议者向这些接受者发送Accept请求，Accept请求附带两个信息：倡议编号n以及倡议值v。倡议值v的选择方式如下：如果在1.2阶段接受者返回了自己曾经接受的具有最高倡议编号Accept请求倡议内容，则从这些倡议内容里面选择倡议编号最高的并将其倡议值作为倡议值v；如果1.2阶段没有接收到任何接受者的Accept请求倡议内容，则可以任意复制给倡议值v。</p>

		<p>2.2【接受者视角】如果接受者接收到了任意倡议编号为n的Accept请求，则接受者接收此请求，除非在此期间接受者响应过具有比n更高编号的Prepare请求。</p>

		<p>通过这两个阶段即可选出唯一的倡议值。对于学习者来说，其需要从接受者那里获知选出的倡议值。方法是：</p></li>
	<li>每当接受者执行完步骤2.2，即接收某个Accept请求后，由其通知所有学习者其接收的倡议。但是这样会导致大量通信因为任意一个接受者都会通知任意一个学习者</li>
	<li>替代策略：从众多学习者选择一个作为代表，由其从接受者那里获知最终被选出的倡议，然后由其通知其他学习者，但是如果这个学习者代表出现故障，则其他学习者无法获知倡议值</li>
	<li>折中方法：选出若干学习者代表，由这些代表从接受者那里获知最终倡议值，然后通知其他学习者</li>
</ul>

<p>通过以上流程，如果有多个并发进程提出各自的倡议值，Paxos可以保证从中选出且只选出一个唯一确定的倡议值，以来来达到副本状态机保持状态一致的目标。</p>

<h3>两阶段提交协议（Two-Phrase Commit，2PC）</h3>

<p>常见的解决分布式事务问题的方式，可以保证在分布式事务中，<strong>要么所有参与进程都提交事务，要么都取消事务</strong>，即实现ACID中的A的常用手段。</p>

<p>在数据一致性环境下，其代表的含义是：要么所有备份数据同时更改某个数值，要么都不改，以此来达到数据的强一致性。</p>

<p>在实际应用中使用两阶段提交协议来作为数据一致性比较少见，更多的是作为<strong>实现数据更新原子性手段</strong>出现。</p>

<p>在两阶段提交的语境下，存在两类不同实体：唯一的协调者（Coordinator）和众多的参与者（Participants）。协调者起到分布式事务的特殊的管理协调作用。</p>

<p>顾名思义两阶段提交将提交过程划分为连续的两个阶段：表决阶段（Voting）和提交阶段（Commit）。</p>

<figure><img src="/Users/xerxes/note/bigDataTec/src/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4.jpg"/></figure>

<ul>
	<li>阶段一：投票阶段

		<ol>
			<li>事务询问：协调者向所有参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应；</li>
			<li>执行事务：各参与者节点执行事务操作，并将undo和redo信息记入事务日志中；</li>
			<li>各参与者向协调者反馈事务询问的请求：如果参与者成功执行了事务操作，那么就反馈给协调者yes响应，表示事务可以执行；如果参与者没有成功执行事务，就反馈给协调者no响应，表示事务不可以执行</li>
		</ol></li>
	<li>阶段二：执行事务提交

		<ul>
			<li>如果协调者从所有参与者获得的反馈都是yes，那么就执行事务提交

				<ol>
					<li>发送提交请求：协调者向所有参与者节点发送commit请求；</li>
					<li>事务提交：参与者收到commit请求，会正式执行事务提交操作，并在完成提交之后释放在整个事务执行期间占用的事务资源；</li>
					<li>反馈事务提交结果：参与者在完成事务提交之后，向协调者发送Ack消息；</li>
					<li>完成事务：协调者接收到所有参与者反馈的Ack消息后，完成事务</li>
				</ol></li>
			<li>如果任意一个参与者向协调者反馈了no响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，则中断事务

				<ol>
					<li>发送回滚请求：协调者向所有参与者节点发出Rollback请求；</li>
					<li>事务回滚：参与者接收到Rollback请求后，会利用其在阶段一中提交的undo信息来执行事务回滚操作，并在完成回滚之后释放在整个事务执行期间占用的资源；</li>
					<li>反馈事务回滚结果：参与者在完成事务回滚之后，向协调者发送Ack消息；</li>
					<li>中断事务：协调者接收到所有参与者反馈的Ack消息后，完成事务中断</li>
				</ol></li>
		</ul></li>
</ul>

<h4>2PC缺点</h4>

<h5>同步阻塞</h5>

<p>2PC协议最明显也是最大的一个问题就是同步阻塞，极大地限制了分布式系统的性能。</p>

<figure><img src="/Users/xerxes/note/bigDataTec/src/%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA.jpg"/></figure>

<p>从两者的有限状态机可以看出，在所有可能状态中，存在3个阻塞状态：协调者的WAIT状态、参与者的INIT状态和READY状态，因为这3个状态需要等待对方的反馈信息。如果一个协议包含阻塞态，则明显是一个很脆弱的系统，因为很可能因为有进程陷入崩溃而导致处于阻塞态的对象进入长时间的等待，系统无法继续向后运行。因此引入两种手段：</p>

<ul>
	<li>超时判断机制：解决协调者WAIT状态和参与者INIT状态的长时间阻塞情形</li>
	<li>参与者互询机制：解决大部分情形下参与者READY状态的长时间阻塞可能</li>
</ul>

<h5>单点问题</h5>

<p>2PC协议整个过程中，协调者起到重要作用。一旦协调者出现问题，整个流程都会有问题。更为严重的是如果协调者在阶段二出现问题，其他参与者将会一直处于锁定事务资源的状态，无法继续完成事务操作。</p>

<h5>数据不一致</h5>

<p>在2PC协议第二阶段，如果协调者向所有参与者发送commit请求的时候发生了网络分区，导致只有一部分参与者收到了commit请求，提交了事务，其他没有收到commit请求的参与者无法提交事务，整个分布式系统就会出现数据不一致的情况。</p>

<h5>过于保守</h5>

<p>如果协调者指示参与者进行事务提交询问的过程中，参与者出现故障而导致协调者始终无法获取到所有参与者的响应信息的话，这时协调者只能依靠其自身的超时机制来判断是否需要中断事务。</p>

<p>2PC协议没有设计较为完善的容错机制，任意一个节点的失败都会导致整个事务的失败。</p>

<h3>三阶段提交协议（Three-Phrase Commit，3PC）</h3>

<h4>协议说明</h4>

<p>3PC是2PC的改进版，其将2PC协议的“提交事务请求”阶段一分为二，形成了由can commit、pre commit和do commit三个阶段组成的事务处理协议。</p>

<figure><img src="/Users/xerxes/note/bigDataTec/src/3PC%E5%8D%8F%E8%AE%AE.jpg"/></figure>

<ul>
	<li>阶段一：can commit

		<ol>
			<li>事务询问：协调者向所有的参与者发送一个包含事务内容的can commit请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应；</li>
			<li>各参与者向协调者反馈事务询问的响应：参与者接收到协调者的can commit请求后，正常情况下如果其自身认为可以顺利执行事务，就反馈yes响应，进入预备状态，否则反馈no响应</li>
		</ol></li>
	<li>阶段二：pre commit

		<ul>
			<li>如果协调者从所有参与者获取的反馈都是yes响应，则执行事务预提交

				<ol>
					<li>发送预提交请求：协调者向所有参与者节点发送pre commit请求，并进入prepared阶段；</li>
					<li>事务预提交：参与者接收到pre commit请求后，执行事务操作，并将undo和redo信息记录到事务日志中；</li>
					<li>各参与者向协调者反馈事务执行的响应：如果参与者成功执行了事务操作，那么就会反馈给协调者Ack响应，同时等待最终指令：commit/abort</li>
				</ol></li>
			<li>如果任意参与者向协调者反馈了no响应或者等待超时，则中断事务

				<ol>
					<li>发送中断请求：协调者向所有参与者发送abort请求</li>
					<li>中断事务：无论是受到来自协调者的abort请求还是等待超时，参与者都会中断事务</li>
				</ol></li>
		</ul></li>
	<li>阶段三：do commit

		<ul>
			<li>如果协调者接收到所有参与者的Ack响应

				<ol>
					<li>发送提交请求：协调者从“预提交”状态转换到“提交”状态，并向所有的参与者发送do commit请求；</li>
					<li>事务提交：参与者接收到do commit请求后，会正式执行事务提交操作，并在完成提交之后释放在整个事务执行期间占用的资源；</li>
					<li>反馈事务提交结果：参与者完成事务提交之后，向协调者发送Ack消息；</li>
					<li>完成事务：协调者接收到所有参与者反馈的Ack消息，完成事务</li>
				</ol></li>
			<li>如果任意参与者向协调者反馈no响应或者等待超时，则中断事务

				<ol>
					<li>发送中断请求：协调者向所有参与者节点发送abort请求；</li>
					<li>事务回滚：参与者接收到abort请求，利用阶段二记录的undo信息来执行事务回滚操作，完成后释放整个事务执行期间占用的资源；</li>
					<li>反馈事务回滚结果：参与者完成事务回滚之后，向协调者发送Ack消息；</li>
					<li>中断事务：协调者接收到所有参与者反馈的Ack消息后，中断事务</li>
				</ol></li>
		</ul></li>
</ul>

<h3>最终一致性</h3>

<figure><img src="/Users/xerxes/note/bigDataTec/src/%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE.jpg"/></figure>

<p>整个流程保证：</p>

<ul>
	<li>业务主动方本地事务提交失败，业务被动方不会收到消息的投递</li>
	<li>只要业务主动方本地事务执行成功，那么消息服务一定会投递给下游的业务被动方，并最终保证业务被动方一定能成功消费该消息</li>
</ul>

<hr />

<h2>分布式系统常见同步机制</h2>

<p>分布式系统为保证数据高可用，需要为数据保存多个副本，随之而来的问题是如何在不同副本间同步数据。</p>

<h3>常见机制</h3>

<figure><img src="/Users/xerxes/note/bigDataTec/src/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%B8%B8%E8%A7%81%E5%90%8C%E6%AD%A5%E6%9C%BA%E5%88%B6.jpg"/></figure>

<ul>
	<li>backup：定期备份，对现有的系统的性能基本没有影响，但节点宕机时只能勉强恢复</li>
	<li>Master-Slave：主从复制，异步复制每个指令，可以看作是粒度更细的定期备份</li>
	<li>Multi-Master：多主，MS的加强版，可以在多个节点上写，事后再想办法同步</li>
	<li>2Phase-Commit：两阶段提交，同步先确保通知到所有节点再写入，性能容易卡在主节点上</li>
	<li>Paxos：类似2PC，同一时刻有多个节点可以写入，只需要通知大多数节点，有更高的吞吐</li>
</ul>

</body>
</html>

